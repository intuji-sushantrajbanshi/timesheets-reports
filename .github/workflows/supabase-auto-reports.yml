name: Export Supabase Data to CSV

on:
  schedule:
    - cron: '15 11 * * *' # Every day at 5 PM Nepal/Kathmandu Time (UTC+5:45)
  workflow_dispatch: # Allow manual trigger
  push:
    branches:
      - master
      - main

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  export-data:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas python-dateutil pytz

      - name: Check Supabase credentials
        run: |
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "‚ùå Supabase credentials are not properly set in GitHub secrets."
            exit 1
          else
            echo "‚úÖ Supabase credentials found"
          fi

      - name: Discover available tables
        run: |
          python -c '
          import os
          import requests
          import json
          
          supabase_url = os.environ["SUPABASE_URL"]
          supabase_key = os.environ["SUPABASE_KEY"]
          
          headers = {
              "apikey": supabase_key,
              "Authorization": f"Bearer {supabase_key}",
              "Content-Type": "application/json"
          }
          
          # Try to get schema information
          try:
              print("üîç Attempting to discover available tables...")
              
              # Try to list tables using the system catalog
              response = requests.get(
                  f"{supabase_url}/rest/v1/",
                  headers=headers
              )
              
              print(f"API root response: {response.status_code}")
              if response.status_code == 200:
                  print(f"Available endpoints: {response.text}")
              
              # Try a few common table names to see if any exist
              test_tables = ["companies", "users", "projects", "time_entries", 
                            "company", "user", "project", "timeentry",
                            "Companies", "Users", "Projects", "TimeEntries"]
              
              for table in test_tables:
                  test_response = requests.get(
                      f"{supabase_url}/rest/v1/{table}?limit=1",
                      headers=headers
                  )
                  
                  if test_response.status_code == 200:
                      print(f"‚úÖ Found table: {table}")
                  else:
                      print(f"‚ùå Table not found: {table} (Status: {test_response.status_code})")
          
          except Exception as e:
              print(f"‚ùå Error during table discovery: {str(e)}")
          '

      - name: Export data from Supabase
        run: |
          mkdir -p exports
          python -c '
          import os
          import requests
          import pandas as pd
          import json
          from datetime import datetime, timedelta
          import pytz
          
          # Configuration
          supabase_url = os.environ["SUPABASE_URL"]
          supabase_key = os.environ["SUPABASE_KEY"]
          
          # Common headers for all requests
          headers = {
              "apikey": supabase_key,
              "Authorization": f"Bearer {supabase_key}",
              "Content-Type": "application/json",
              "Prefer": "return=representation"
          }
          
          def fetch_data(table_name, query_params=None, select_fields="*"):
              """Fetch data from Supabase table and return as DataFrame"""
              endpoint = f"{supabase_url}/rest/v1/{table_name}"
              
              params = {}
              if select_fields != "*":
                  params["select"] = select_fields
                  
              if query_params:
                  params.update(query_params)
                  
              try:
                  print(f"üîç Fetching from {endpoint} with params {params}")
                  response = requests.get(endpoint, headers=headers, params=params)
                  
                  if response.status_code == 200:
                      data = response.json()
                      if not data:
                          print(f"No data returned for {table_name}")
                          return pd.DataFrame()
                          
                      df = pd.DataFrame(data)
                      print(f"‚úÖ Successfully fetched {len(df)} rows from {table_name}")
                      return df
                  else:
                      print(f"‚ùå Error fetching {table_name}: {response.status_code} - {response.text}")
                      return pd.DataFrame()
                      
              except Exception as e:
                  print(f"‚ùå Error fetching {table_name}: {str(e)}")
                  return pd.DataFrame()
          
          # Calculate date for filtering time entries (last 30 days)
          thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
          current_date = datetime.now().strftime("%Y-%m-%d")
          
          # Try different table name variations
          table_variations = {
              "companies": ["companies", "company", "Companies", "Company"],
              "projects": ["projects", "project", "Projects", "Project"],
              "users": ["users", "user", "Users", "User"],
              "activity_types": ["activity_types", "activitytype", "activitytypes", "ActivityType", "ActivityTypes"],
              "focus_areas": ["focus_areas", "focusarea", "focusareas", "FocusArea", "FocusAreas"],
              "time_entries": ["time_entries", "timeentry", "timeentries", "TimeEntry", "TimeEntries"],
              "submissions": ["submissions", "submission", "Submissions", "Submission"]
          }
          
          # Export data for each table
          results = {}
          
          # Try to find and export each table
          for display_name, variations in table_variations.items():
              found = False
              
              for table_name in variations:
                  # Try without deletedAt filter first to see if table exists
                  test_df = fetch_data(table_name, {"limit": 1})
                  
                  if not test_df.empty:
                      print(f"‚úÖ Found table: {table_name}")
                      found = True
                      
                      # Now fetch with proper filters
                      if display_name == "time_entries":
                          df = fetch_data(table_name, {"deletedAt": "is.null", "entryDate": f"gte.{thirty_days_ago}"})
                      else:
                          df = fetch_data(table_name, {"deletedAt": "is.null"})
                      
                      results[display_name] = len(df) if not df.empty else 0
                      
                      if not df.empty:
                          # Remove sensitive fields for users
                          if display_name == "users" and "password" in df.columns:
                              df = df.drop(columns=["password"])
                          
                          df.to_csv(f"exports/{display_name}.csv", index=False)
                      
                      break
              
              if not found:
                  print(f"‚ùå Could not find any variation of {display_name}")
                  results[display_name] = 0
          
          # Create a summary file with export information
          with open("exports/export_summary.json", "w") as f:
              summary = {
                  "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                  "tables_exported": results,
                  "time_entries_period": f"{thirty_days_ago} to {current_date}"
              }
              json.dump(summary, f, indent=2)
          
          # Get current timestamp for HTML
          timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
          
          # Create an index.html file for GitHub Pages
          html_content = f"""<!DOCTYPE html>
          <html>
          <head>
              <title>Supabase Data Exports</title>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1">
              <style>
                  body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; line-height: 1.6; }}
                  h1 {{ color: #333; }}
                  .container {{ max-width: 800px; margin: 0 auto; }}
                  table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                  th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                  th {{ background-color: #f2f2f2; }}
                  .download-btn {{ display: inline-block; background-color: #4CAF50; color: white; 
                                  padding: 8px 16px; text-decoration: none; border-radius: 4px; }}
                  .download-btn:hover {{ background-color: #45a049; }}
                  .timestamp {{ color: #666; font-style: italic; }}
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>Supabase Data Exports</h1>
                  <p class="timestamp">Last updated: {timestamp} UTC</p>
                  
                  <table>
                      <tr>
                          <th>Dataset</th>
                          <th>Records</th>
                          <th>Download</th>
                      </tr>"""
          
          # Add table rows dynamically based on what was successfully exported
          for table_name, count in results.items():
              file_name = f"{table_name}.csv"
              display_name = table_name.replace("_", " ").title()
              
              if table_name == "time_entries":
                  display_name += " (Last 30 Days)"
                  
              download_link = f"""
                      <tr>
                          <td>{display_name}</td>
                          <td>{count}</td>
                          <td>"""
              
              if count > 0:
                  download_link += f"""<a href="{file_name}" class="download-btn">Download CSV</a>"""
              else:
                  download_link += """<span style="color: #999;">No data</span>"""
                  
              download_link += """</td>
                      </tr>"""
              
              html_content += download_link
          
          # Complete the HTML
          html_content += f"""
                  </table>
                  
                  <h2>Export Summary</h2>
                  <p>Time period for time entries: {thirty_days_ago} to {current_date}</p>
                  <p><a href="export_summary.json">View detailed export summary (JSON)</a></p>
              </div>
          </body>
          </html>"""
          
          with open("exports/index.html", "w") as f:
              f.write(html_content)
          
          # Check if we have any successful exports
          successful_exports = sum(1 for count in results.values() if count > 0)
          if successful_exports == 0:
              print("‚ö†Ô∏è Warning: No data was successfully exported from any table")
              print("Please check your Supabase configuration and table access permissions")
              
              # Create a placeholder file to indicate we tried
              with open("exports/README.md", "w") as f:
                  f.write(f"""# Supabase Export Attempt

No data could be exported on {timestamp}.

Possible reasons:
1. Table names might be different from what we tried
2. API key might not have sufficient permissions
3. Row Level Security (RLS) policies might be restricting access
4. Tables might be in a different schema than "public"

Please check your Supabase configuration and try again.
""")
          else:
              print(f"‚úÖ Successfully exported data from {successful_exports} tables")
          '

      - name: Check for exported files
        id: check_exports
        run: |
          if [ -f exports/export_summary.json ]; then
            echo "Exports summary file exists"
            cat exports/export_summary.json
            echo "export_exists=true" >> $GITHUB_OUTPUT
          else
            echo "No exports were generated"
            echo "export_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload CSV artifacts
        uses: actions/upload-artifact@v4
        with:
          name: supabase-data-exports-${{ github.run_number }}
          path: exports/
          retention-days: 30

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: exports
          publish_branch: gh-pages
          keep_files: false
          commit_message: "üìä Update Supabase data exports - ${{ github.run_number }}"
