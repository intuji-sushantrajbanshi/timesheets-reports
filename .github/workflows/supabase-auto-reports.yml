name: Export Supabase Data to CSV

on:
  schedule:
    - cron: '15 11 * * *' # Every day at 5 PM Nepal/Kathmandu Time (UTC+5:45)
  workflow_dispatch:
  push:
    branches:
      - master
      - main

permissions:
  contents: write

jobs:
  export-data:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas python-dateutil pytz

      - name: Check Supabase credentials
        run: |
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "âŒ Supabase credentials are not properly set in GitHub secrets."
            exit 1
          else
            echo "âœ… Supabase credentials found"
          fi

      - name: Export project time data from Supabase
        run: |
          mkdir -p exports
          python -c '
          import os
          import requests
          import pandas as pd
          import json
          from datetime import datetime
          
          # Configuration
          supabase_url = os.environ["SUPABASE_URL"]
          supabase_key = os.environ["SUPABASE_KEY"]
          
          # Common headers for all requests
          headers = {
              "apikey": supabase_key,
              "Authorization": f"Bearer {supabase_key}",
              "Content-Type": "application/json",
              "Prefer": "return=representation"
          }
          
          def fetch_data(table_name, query_params=None, select_fields="*"):
              endpoint = f"{supabase_url}/rest/v1/{table_name}"
              
              params = {}
              if select_fields != "*":
                  params["select"] = select_fields
                  
              if query_params:
                  params.update(query_params)
                  
              try:
                  print(f"ðŸ” Fetching from {endpoint} with params {params}")
                  response = requests.get(endpoint, headers=headers, params=params)
                  
                  if response.status_code == 200:
                      data = response.json()
                      if not data:
                          print(f"No data returned for {table_name}")
                          return pd.DataFrame()
                          
                      df = pd.DataFrame(data)
                      print(f"âœ… Successfully fetched {len(df)} rows from {table_name}")
                      return df
                  else:
                      print(f"âŒ Error fetching {table_name}: {response.status_code} - {response.text}")
                      return pd.DataFrame()
                      
              except Exception as e:
                  print(f"âŒ Error fetching {table_name}: {str(e)}")
                  return pd.DataFrame()
          
          # Get company ID for "Timesheet App (Production)"
          print("ðŸ” Fetching company ID for Timesheet App (Production)")
          company_df = fetch_data("Company", {"name": "eq.Timesheet App (Production)"}, "id")
          
          if company_df.empty or "id" not in company_df.columns:
              print("âŒ Company not found")
              # Create error file instead of returning
              with open("exports/error.txt", "w") as f:
                  f.write("Company 'Timesheet App (Production)' not found")
              exit(1)  # Exit with error code instead of return
              
          company_id = company_df.iloc[0]["id"]
          print(f"âœ… Company ID: {company_id}")
          
          # Create project time report
          try:
              # Fetch data from required tables with company filter
              print("ðŸ“Š Fetching time entries...")
              time_entries_df = fetch_data(
                  "TimeEntry",
                  {
                      "companyId": f"eq.{company_id}",
                      "deletedAt": "is.null"
                  }
              )
              
              print("ðŸ“Š Fetching projects...")  
              projects_df = fetch_data(
                  "Project",
                  {
                      "companyId": f"eq.{company_id}",
                      "deletedAt": "is.null"
                  }
              )
              
              print("ðŸ“Š Fetching users...")
              users_df = fetch_data(
                  "User",
                  {
                      "companyId": f"eq.{company_id}",
                      "deletedAt": "is.null"
                  }
              )
              
              if time_entries_df.empty or projects_df.empty or users_df.empty:
                  print("âŒ One or more required tables are empty")
                  return False
              
              # Print column names for debugging
              print(f"ðŸ“‹ TimeEntry columns: {list(time_entries_df.columns)}")
              print(f"ðŸ“‹ Project columns: {list(projects_df.columns)}")
              print(f"ðŸ“‹ User columns: {list(users_df.columns)}")
              
              # Find column names (case-insensitive matching)
              def find_column(df, possible_names):
                  df_columns = [col.lower() for col in df.columns]
                  for name in possible_names:
                      if name.lower() in df_columns:
                          return df.columns[df_columns.index(name.lower())]
                  return None
              
              # Map column names for projects
              project_id_col = find_column(projects_df, ["id"])
              project_title_col = find_column(projects_df, ["title"])
              
              # Map column names for users
              user_id_col = find_column(users_df, ["id"])
              user_first_name_col = find_column(users_df, ["firstName"])
              user_last_name_col = find_column(users_df, ["lastName"])
              user_email_col = find_column(users_df, ["email"])
              
              # Map column names for time entries
              time_project_id_col = find_column(time_entries_df, ["projectId"])
              time_user_id_col = find_column(time_entries_df, ["userId"])
              time_duration_col = find_column(time_entries_df, ["duration"])
              time_entry_date_col = find_column(time_entries_df, ["entryDate", "created_at"])
              
              print(f"ðŸ” Column mapping:")
              print(f"   Project: id={project_id_col}, title={project_title_col}")
              print(f"   User: id={user_id_col}, first={user_first_name_col}, last={user_last_name_col}, email={user_email_col}")
              print(f"   Time: project_id={time_project_id_col}, user_id={time_user_id_col}, duration={time_duration_col}")
              
              # Filter projects for only the two we want
              target_projects = [
                  "Department of Health (Government of Western Australia)",
                  "Coerco"
              ]
              
              projects_filtered = projects_df[
                  projects_df[project_title_col].isin(target_projects)
              ]
              
              if projects_filtered.empty:
                  print("âŒ Target projects not found")
                  print(f"Available projects: {projects_df[project_title_col].tolist()}")
                  return False
              
              print(f"âœ… Found {len(projects_filtered)} target projects")
              
              # Merge time entries with filtered projects
              merged_df = time_entries_df.merge(
                  projects_filtered[[project_id_col, project_title_col]], 
                  left_on=time_project_id_col, 
                  right_on=project_id_col, 
                  how="inner"
              )
              
              # Merge with users
              user_cols_to_merge = [user_id_col]
              if user_first_name_col: user_cols_to_merge.append(user_first_name_col)
              if user_last_name_col: user_cols_to_merge.append(user_last_name_col)
              if user_email_col: user_cols_to_merge.append(user_email_col)
              
              merged_df = merged_df.merge(
                  users_df[user_cols_to_merge], 
                  left_on=time_user_id_col, 
                  right_on=user_id_col, 
                  how="inner"
              )
              
              if merged_df.empty:
                  print("âŒ No data after merging tables")
                  return False
              
              print(f"âœ… Successfully merged data: {len(merged_df)} time entries")
              
              # Create user name column
              if user_first_name_col and user_last_name_col:
                  merged_df["user_name"] = (
                      merged_df[user_first_name_col].fillna("").astype(str) + " " + 
                      merged_df[user_last_name_col].fillna("").astype(str)
                  ).str.strip()
              else:
                  merged_df["user_name"] = merged_df[user_email_col] if user_email_col else "Unknown User"
              
              # Calculate duration
              if time_duration_col and time_duration_col in merged_df.columns:
                  merged_df["duration_minutes"] = pd.to_numeric(merged_df[time_duration_col], errors="coerce").fillna(0)
              else:
                  # Calculate from start/end times if available
                  start_col = find_column(time_entries_df, ["startTime"])
                  end_col = find_column(time_entries_df, ["endTime"])
                  
                  if start_col and end_col:
                      merged_df["duration_minutes"] = (
                          pd.to_datetime(merged_df[end_col]) - 
                          pd.to_datetime(merged_df[start_col])
                      ).dt.total_seconds() / 60
                  else:
                      merged_df["duration_minutes"] = 0
                      print("âš ï¸ No duration column found, using 0 for all entries")
              
              # Group by project and user
              group_cols = [project_title_col, "user_name"]
              if user_email_col:
                  group_cols.append(user_email_col)
              
              agg_dict = {
                  "duration_minutes": ["sum", "count"]
              }
              
              if time_entry_date_col:
                  agg_dict[time_entry_date_col] = ["min", "max"]
              
              grouped = merged_df.groupby(group_cols).agg(agg_dict).reset_index()
              
              # Flatten column names
              new_columns = []
              for col in grouped.columns:
                  if isinstance(col, tuple):
                      if col[1]:
                          new_columns.append(f"{col[0]}_{col[1]}")
                      else:
                          new_columns.append(col[0])
                  else:
                      new_columns.append(col)
              grouped.columns = new_columns
              
              # Rename columns to match expected format
              column_mapping = {
                  project_title_col: "project",
                  "user_name": "user_name",
                  "duration_minutes_sum": "total_duration_raw",
                  "duration_minutes_count": "total_entries"
              }
              
              if user_email_col:
                  column_mapping[user_email_col] = "user_email"
              
              if time_entry_date_col:
                  column_mapping[f"{time_entry_date_col}_min"] = "first_entry_date"
                  column_mapping[f"{time_entry_date_col}_max"] = "last_entry_date"
              
              grouped = grouped.rename(columns=column_mapping)
              
              # Add total_hours column
              grouped["total_hours"] = grouped["total_duration_raw"]
              
              # Add missing columns if not present
              required_cols = ["project", "user_name", "user_email", "total_duration_raw", "total_hours", "total_entries", "first_entry_date", "last_entry_date"]
              for col in required_cols:
                  if col not in grouped.columns:
                      grouped[col] = "" if col in ["user_email", "first_entry_date", "last_entry_date"] else 0
              
              # Sort data: Department of Health first (by total_hours desc), then separator, then Coerco (by total_hours desc)
              dept_health = grouped[grouped["project"] == "Department of Health (Government of Western Australia)"].copy()
              coerco = grouped[grouped["project"] == "Coerco"].copy()
              
              # Sort each project group by total_hours descending
              dept_health = dept_health.sort_values("total_hours", ascending=False)
              coerco = coerco.sort_values("total_hours", ascending=False)
              
              # Create final report with separator
              final_rows = []
              
              # Add Department of Health data
              for _, row in dept_health.iterrows():
                  final_rows.append(row.to_dict())
              
              # Add separator row
              separator_row = {
                  "project": "--- SEPARATOR ---",
                  "user_name": "",
                  "user_email": "",
                  "total_duration_raw": None,
                  "total_hours": None,
                  "total_entries": None,
                  "first_entry_date": None,
                  "last_entry_date": None
              }
              final_rows.append(separator_row)
              
              # Add Coerco data
              for _, row in coerco.iterrows():
                  final_rows.append(row.to_dict())
              
              # Create final DataFrame
              final_df = pd.DataFrame(final_rows)
              
              # Ensure column order matches the expected format
              column_order = ["project", "user_name", "user_email", "total_duration_raw", "total_hours", "total_entries", "first_entry_date", "last_entry_date"]
              final_df = final_df[column_order]
              
              # Save to CSV
              final_df.to_csv("exports/project_time_report.csv", index=False)
              print(f"âœ… Created project time report with {len(final_df)} rows")
              
              # Create summary
              summary = {
                  "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                  "company_id": company_id,
                  "total_rows": len(final_df),
                  "dept_health_users": len(dept_health),
                  "coerco_users": len(coerco)
              }
              
              with open("exports/export_summary.json", "w") as f:
                  json.dump(summary, f, indent=2)
              
              return True
                  
          except Exception as e:
              print(f"âŒ Error creating project time report: {str(e)}")
              import traceback
              traceback.print_exc()
              return False
          '

      - name: Check for exported files
        run: |
          echo "ðŸ“ Contents of exports directory:"
          ls -la exports/ || echo "No exports directory found"
          
          if [ -d "exports" ] && [ "$(ls -A exports)" ]; then
            echo "âœ… Export files found"
          else
            echo "âš ï¸ No export files found, creating placeholder"
            mkdir -p exports
            echo "No data available" > exports/no-data.txt
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: supabase-project-time-export
          path: exports/
          retention-days: 30

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add exports/
          git diff --staged --quiet || git commit -m "Update project time export - $(date)"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
