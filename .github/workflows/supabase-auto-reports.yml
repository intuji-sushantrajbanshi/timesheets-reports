name: Export Supabase Data to CSV

on:
  schedule:
    - cron: '15 11 * * *' # Every day at 5 PM Nepal/Kathmandu Time (UTC+5:45)
  workflow_dispatch:
  push:
    branches:
      - master
      - main

permissions:
  contents: write

jobs:
  export-data:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas python-dateutil pytz

      - name: Check Supabase credentials
        run: |
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "❌ Supabase credentials are not properly set in GitHub secrets."
            exit 1
          else
            echo "✅ Supabase credentials found"
          fi

      - name: Create Python export script
        run: |
          mkdir -p exports
          cat > export_script.py << 'EOF'
import os
import requests
import pandas as pd
import json
from datetime import datetime, timedelta
import sys

def main():
    try:
        # Configuration
        supabase_url = os.environ["SUPABASE_URL"]
        supabase_key = os.environ["SUPABASE_KEY"]
        
        # Common headers for all requests
        headers = {
            "apikey": supabase_key,
            "Authorization": f"Bearer {supabase_key}",
            "Content-Type": "application/json",
            "Prefer": "return=representation"
        }
        
        # First, list all tables to check the correct case
        print("🔍 Listing available tables...")
        response = requests.get(
            f"{supabase_url}/rest/v1/",
            headers=headers
        )
        
        if response.status_code == 200:
            print(f"✅ Successfully connected to Supabase")
        else:
            print(f"❌ Failed to connect to Supabase: {response.status_code} - {response.text}")
        
        # Function to fetch data with proper error handling
        def fetch_data(table_name, query_params=None, select_fields="*"):
            endpoint = f"{supabase_url}/rest/v1/{table_name}"
            
            params = {}
            if select_fields != "*":
                params["select"] = select_fields
                
            if query_params:
                params.update(query_params)
                
            try:
                print(f"🔍 Fetching from {endpoint} with params {params}")
                response = requests.get(endpoint, headers=headers, params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    if not data:
                        print(f"No data returned for {table_name}")
                        return pd.DataFrame()
                        
                    df = pd.DataFrame(data)
                    print(f"✅ Successfully fetched {len(df)} rows from {table_name}")
                    return df
                else:
                    print(f"❌ Error fetching {table_name}: {response.status_code} - {response.text}")
                    return pd.DataFrame()
                    
            except Exception as e:
                print(f"❌ Error fetching {table_name}: {str(e)}")
                return pd.DataFrame()
        
        # Try both capitalized and lowercase table names
        print("🔍 Fetching company ID for Timesheet App (Production)")
        
        # Try lowercase first (more common in PostgreSQL)
        company_df = fetch_data("company", {"name": "eq.Timesheet App (Production)"}, "id")
        
        if company_df.empty or "id" not in company_df.columns:
            # Try with capitalization as in the schema
            print("⚠️ Trying with capitalized table name...")
            company_df = fetch_data("Company", {"name": "eq.Timesheet App (Production)"}, "id")
        
        if company_df.empty or "id" not in company_df.columns:
            print("❌ Company not found")
            with open("exports/error.txt", "w") as f:
                f.write("Company 'Timesheet App (Production)' not found. Check table names in Supabase.")
            sys.exit(1)
            
        company_id = company_df.iloc[0]["id"]
        print(f"✅ Company ID: {company_id}")
        
        # Determine correct table names for the rest of the query
        # Try both variants for TimeEntry
        time_entries_df = fetch_data(
            "timeentry",
            {
                "companyId": f"eq.{company_id}",
                "deletedAt": "is.null"
            }
        )
        
        if time_entries_df.empty:
            time_entries_df = fetch_data(
                "TimeEntry",
                {
                    "companyId": f"eq.{company_id}",
                    "deletedAt": "is.null"
                }
            )
        
        # Based on the successful table name pattern, use the same for other tables
        table_name_style = "lowercase" if "timeentry" in time_entries_df.columns else "capitalized"
        print(f"✅ Using {table_name_style} table names")
        
        # Map the table names based on the detected style
        table_names = {
            "project": "project" if table_name_style == "lowercase" else "Project",
            "user": "user" if table_name_style == "lowercase" else "User",
            "focusarea": "focusarea" if table_name_style == "lowercase" else "FocusArea",
            "activitytype": "activitytype" if table_name_style == "lowercase" else "ActivityType",
            "timeentry": "timeentry" if table_name_style == "lowercase" else "TimeEntry"
        }
        
        # Configuration for the query (matching your SQL query structure)
        # You can modify these values as needed
        TARGET_PROJECT = "LFA First Response"  # From your SQL example
        DATE_FILTER = "TODAY"  # Options: TILL_DATE, TODAY, THIS_WEEK, LAST_WEEK, THIS_MONTH, LAST_MONTH
        
        print(f"📊 Extracting data for project: {TARGET_PROJECT}")
        print(f"📅 Date filter: {DATE_FILTER}")
        
        # Calculate date range based on filter
        today = datetime.now().date()
        
        if DATE_FILTER == "TODAY":
            start_date = today
            end_date = today
        elif DATE_FILTER == "THIS_WEEK":
            # Monday to Sunday (0 = Monday in datetime.weekday())
            start_date = today - timedelta(days=today.weekday())
            end_date = start_date + timedelta(days=6)
        elif DATE_FILTER == "LAST_WEEK":
            # Previous Monday to Sunday
            start_date = today - timedelta(days=today.weekday() + 7)
            end_date = start_date + timedelta(days=6)
        elif DATE_FILTER == "THIS_MONTH":
            start_date = today.replace(day=1)
            # Get last day of current month
            if today.month == 12:
                end_date = today.replace(year=today.year + 1, month=1, day=1) - timedelta(days=1)
            else:
                next_month = today.month + 1
                end_date = today.replace(month=next_month, day=1) - timedelta(days=1)
        elif DATE_FILTER == "LAST_MONTH":
            if today.month == 1:
                start_date = today.replace(year=today.year - 1, month=12, day=1)
            else:
                start_date = today.replace(month=today.month - 1, day=1)
            end_date = today.replace(day=1) - timedelta(days=1)
        else:  # TILL_DATE
            start_date = datetime(1900, 1, 1).date()
            end_date = datetime(2100, 12, 31).date()
        
        print(f"📅 Date range: {start_date} to {end_date}")
        
        # Fetch data with proper table names
        print("📊 Fetching projects...")
        projects_df = fetch_data(
            table_names["project"],
            {
                "companyId": f"eq.{company_id}",
                "deletedAt": "is.null",
                "title": f"eq.{TARGET_PROJECT}"
            }
        )
        
        print("📊 Fetching users...")
        users_df = fetch_data(
            table_names["user"],
            {
                "companyId": f"eq.{company_id}",
                "deletedAt": "is.null"
            }
        )
        
        print("📊 Fetching focus areas...")
        focus_areas_df = fetch_data(
            table_names["focusarea"],
            {
                "companyId": f"eq.{company_id}",
                "deletedAt": "is.null"
            }
        )
        
        print("📊 Fetching activity types...")
        activity_types_df = fetch_data(
            table_names["activitytype"],
            {
                "companyId": f"eq.{company_id}",
                "deletedAt": "is.null"
            }
        )
        
        print("📊 Fetching time entries...")
        # For time entries, we need to handle the date filtering
        # Since we can't use complex filters like BETWEEN in a single query parameter,
        # we'll fetch all entries and filter them after
        time_entries_df = fetch_data(
            table_names["timeentry"],
            {
                "companyId": f"eq.{company_id}",
                "deletedAt": "is.null"
            }
        )
        
        # Check if we have all required data
        if time_entries_df.empty:
            print("❌ No time entries found")
            with open("exports/error.txt", "w") as f:
                f.write(f"No time entries found for company ID: {company_id}")
            sys.exit(1)
            
        if projects_df.empty:
            print("❌ Target project not found")
            with open("exports/error.txt", "w") as f:
                f.write(f"Project '{TARGET_PROJECT}' not found")
            sys.exit(1)
            
        # Identify column names (they might be case-sensitive)
        def find_column(df, possible_names):
            for col in df.columns:
                for name in possible_names:
                    if col.lower() == name.lower():
                        return col
            return None
        
        # Map column names
        project_id_col = find_column(projects_df, ["id"])
        project_title_col = find_column(projects_df, ["title"])
        
        user_id_col = find_column(users_df, ["id"])
        user_first_name_col = find_column(users_df, ["firstname", "firstName"])
        user_last_name_col = find_column(users_df, ["lastname", "lastName"])
        user_email_col = find_column(users_df, ["email"])
        
        focus_id_col = find_column(focus_areas_df, ["id"])
        focus_title_col = find_column(focus_areas_df, ["title"])
        focus_color_col = find_column(focus_areas_df, ["colourcode", "colourCode"])
        
        activity_id_col = find_column(activity_types_df, ["id"])
        activity_title_col = find_column(activity_types_df, ["title"])
        
        time_id_col = find_column(time_entries_df, ["id"])
        time_project_id_col = find_column(time_entries_df, ["projectid", "projectId"])
        time_user_id_col = find_column(time_entries_df, ["userid", "userId"])
        time_focus_id_col = find_column(time_entries_df, ["focusareaid", "focusAreaId"])
        time_activity_id_col = find_column(time_entries_df, ["activitytypeid", "activityTypeId"])
        time_entry_date_col = find_column(time_entries_df, ["entrydate", "entryDate"])
        time_start_col = find_column(time_entries_df, ["starttime", "startTime"])
        time_end_col = find_column(time_entries_df, ["endtime", "endTime"])
        time_duration_col = find_column(time_entries_df, ["duration"])
        
        print("📋 Mapped columns:")
        print(f"  Project: id={project_id_col}, title={project_title_col}")
        print(f"  User: id={user_id_col}, first={user_first_name_col}, last={user_last_name_col}")
        print(f"  Time entry: date={time_entry_date_col}, projectId={time_project_id_col}")
        
        # Convert entryDate column to datetime for filtering
        time_entries_df[time_entry_date_col] = pd.to_datetime(time_entries_df[time_entry_date_col]).dt.date
        
        # Filter time entries by date
        filtered_entries = time_entries_df[
            (time_entries_df[time_entry_date_col] >= start_date) & 
            (time_entries_df[time_entry_date_col] <= end_date)
        ]
        
        # Get the project ID
        project_id = projects_df.iloc[0][project_id_col]
        
        # Filter time entries for the target project
        project_entries = filtered_entries[filtered_entries[time_project_id_col] == project_id]
        
        if project_entries.empty:
            print(f"❌ No time entries found for project '{TARGET_PROJECT}' in the selected date range")
            with open("exports/error.txt", "w") as f:
                f.write(f"No time entries found for project '{TARGET_PROJECT}' between {start_date} and {end_date}")
            sys.exit(1)
            
        print(f"✅ Found {len(project_entries)} time entries for target project in date range")
        
        # Merge with users
        result_df = project_entries.merge(
            users_df, 
            left_on=time_user_id_col, 
            right_on=user_id_col,
            how="inner"
        )
        
        # Merge with focus areas
        result_df = result_df.merge(
            focus_areas_df,
            left_on=time_focus_id_col,
            right_on=focus_id_col,
            how="inner",
            suffixes=("", "_focus")
        )
        
        # Merge with activity types
        result_df = result_df.merge(
            activity_types_df,
            left_on=time_activity_id_col,
            right_on=activity_id_col,
            how="inner",
            suffixes=("", "_activity")
        )
        
        # Create user_name column
        result_df["user_name"] = result_df[user_first_name_col] + " " + result_df[user_last_name_col]
        
        # Calculate duration in hours
        if time_duration_col:
            # If duration is stored directly
            result_df["total_duration_raw"] = result_df[time_duration_col]
        else:
            # Calculate from start/end times
            result_df["startTime_dt"] = pd.to_datetime(result_df[time_start_col])
            result_df["endTime_dt"] = pd.to_datetime(result_df[time_end_col])
            result_df["total_duration_raw"] = (
                result_df["endTime_dt"] - result_df["startTime_dt"]
            ).dt.total_seconds() / 60  # Duration in minutes
        
        # Group by user, focus area, and activity type
        agg_df = result_df.groupby([
            project_title_col,
            "user_name",
            user_email_col,
            focus_title_col + "_focus" if "_focus" in focus_title_col else focus_title_col,
            focus_color_col,
            activity_title_col + "_activity" if "_activity" in activity_title_col else activity_title_col
        ]).agg({
            "total_duration_raw": "sum",
            time_id_col: "count",
            time_entry_date_col: ["min", "max"]
        }).reset_index()
        
        # Flatten multi-level column names
        agg_df.columns = [
            col[0] if isinstance(col, tuple) and col[1] == "" else 
            f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col 
            for col in agg_df.columns
        ]
        
        # Rename columns to match SQL query output
        column_mapping = {
            project_title_col: "project_name",
            user_email_col: "user_email",
            focus_title_col + "_focus" if "_focus" in focus_title_col else focus_title_col: "focus_area",
            focus_color_col: "focus_area_color",
            activity_title_col + "_activity" if "_activity" in activity_title_col else activity_title_col: "activity_type",
            time_id_col + "_count": "total_entries",
            time_entry_date_col + "_min": "first_entry_date",
            time_entry_date_col + "_max": "last_entry_date"
        }
        
        agg_df = agg_df.rename(columns=column_mapping)
        
        # Calculate total hours (duration in minutes / 60)
        agg_df["total_hours"] = agg_df["total_duration_raw"] / 60
        
        # Add filter information
        agg_df["applied_date_filter"] = DATE_FILTER
        agg_df["filter_start_date"] = start_date
        agg_df["filter_end_date"] = end_date
        
        # Order columns to match SQL query output
        final_columns = [
            "applied_date_filter",
            "filter_start_date",
            "filter_end_date",
            "project_name",
            "user_name",
            "user_email",
            "focus_area",
            "focus_area_color",
            "activity_type",
            "total_duration_raw",
            "total_hours",
            "total_entries",
            "first_entry_date",
            "last_entry_date"
        ]
        
        # Ensure all columns exist (add missing ones with empty values)
        for col in final_columns:
            if col not in agg_df.columns:
                agg_df[col] = None
        
        final_df = agg_df[final_columns]
        
        # Sort by user name, focus area, and total hours (descending)
        final_df = final_df.sort_values([
            "user_name",
            "focus_area",
            "total_hours"
        ], ascending=[True, True, False])
        
        # Save to CSV
        output_filename = f"project_time_report_{TARGET_PROJECT.replace(' ', '_')}_{DATE_FILTER}.csv"
        final_df.to_csv(f"exports/{output_filename}", index=False)
        
        print(f"✅ Created project time report: {output_filename}")
        print(f"   Records: {len(final_df)}")
        
        # Create summary
        summary = {
            "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "company_id": company_id,
            "project": TARGET_PROJECT,
            "date_filter": DATE_FILTER,
            "date_range": f"{start_date} to {end_date}",
            "records": len(final_df)
        }
        
        with open("exports/export_summary.json", "w") as f:
            json.dump(summary, f, indent=2)
        
        return True
        
    except Exception as e:
        print(f"❌ Error: {str(e)}")
        import traceback
        traceback.print_exc()
        
        with open("exports/error.txt", "w") as f:
            f.write(f"Error during export: {str(e)}\n\n")
            f.write(traceback.format_exc())
        
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)
EOF

      - name: Export project time data from Supabase
        run: python export_script.py

      - name: Check for exported files
        run: |
          echo "📁 Contents of exports directory:"
          ls -la exports/ || echo "No exports directory found"
          
          if [ -d "exports" ] && [ "$(ls -A exports)" ]; then
            echo "✅ Export files found"
          else
            echo "⚠️ No export files found, creating placeholder"
            mkdir -p exports
            echo "No data available" > exports/no-data.txt
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: supabase-project-time-export
          path: exports/
          retention-days: 30

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add exports/
          git diff --staged --quiet || git commit -m "Update project time export - $(date)"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
