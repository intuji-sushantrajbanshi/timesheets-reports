name: Export Supabase Data to CSV

on:
  schedule:
    - cron: '15 11 * * *' # Every day at 5 PM Nepal/Kathmandu Time (UTC+5:45)
  workflow_dispatch:
  push:
    branches:
      - master
      - main

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  export-data:
    runs-on: ubuntu-latest
    # Remove the environment block temporarily or make it conditional
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas python-dateutil pytz

      - name: Check Supabase credentials
        run: |
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "‚ùå Supabase credentials are not properly set in GitHub secrets."
            exit 1
          else
            echo "‚úÖ Supabase credentials found"
          fi

      - name: Export data from Supabase
        run: |
          mkdir -p exports
          python -c '
          import os
          import requests
          import pandas as pd
          import json
          from datetime import datetime, timedelta
          
          # Configuration
          supabase_url = os.environ["SUPABASE_URL"]
          supabase_key = os.environ["SUPABASE_KEY"]
          
          # Common headers for all requests
          headers = {
              "apikey": supabase_key,
              "Authorization": f"Bearer {supabase_key}",
              "Content-Type": "application/json",
              "Prefer": "return=representation"
          }
          
          def fetch_data(table_name, query_params=None, select_fields="*"):
              endpoint = f"{supabase_url}/rest/v1/{table_name}"
              
              params = {}
              if select_fields != "*":
                  params["select"] = select_fields
                  
              if query_params:
                  params.update(query_params)
                  
              try:
                  print(f"üîç Fetching from {endpoint} with params {params}")
                  response = requests.get(endpoint, headers=headers, params=params)
                  
                  if response.status_code == 200:
                      data = response.json()
                      if not data:
                          print(f"No data returned for {table_name}")
                          return pd.DataFrame()
                          
                      df = pd.DataFrame(data)
                      print(f"‚úÖ Successfully fetched {len(df)} rows from {table_name}")
                      return df
                  else:
                      print(f"‚ùå Error fetching {table_name}: {response.status_code} - {response.text}")
                      return pd.DataFrame()
                      
              except Exception as e:
                  print(f"‚ùå Error fetching {table_name}: {str(e)}")
                  return pd.DataFrame()
          
          # Calculate date for filtering time entries (last 30 days)
          thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
          current_date = datetime.now().strftime("%Y-%m-%d")
          
          # Function to get all available tables
          def get_available_tables():
              """Get list of all available tables from Supabase"""
              try:
                  # Try to get schema information
                  response = requests.get(f"{supabase_url}/rest/v1/", headers=headers)
                  if response.status_code == 200:
                      # Parse the OpenAPI spec to get table names
                      openapi_data = response.json()
                      if "paths" in openapi_data:
                          tables = []
                          for path in openapi_data["paths"].keys():
                              if path.startswith("/") and not path.startswith("/rpc/"):
                                  table_name = path.strip("/")
                                  if table_name and "." not in table_name:
                                      tables.append(table_name)
                          return tables
              except Exception as e:
                  print(f"Could not fetch table list: {e}")
              
              # Fallback: common lowercase table names
              return ["companies", "projects", "users", "activity_types", "focus_areas", "time_entries", "submissions"]
          
          # Get available tables
          available_tables = get_available_tables()
          print(f"üîç Available tables: {available_tables}")
          
          # Map common table names to what might exist in your database
          table_mapping = {
              "companies": ["companies", "company", "Company"],
              "projects": ["projects", "project", "Project"], 
              "users": ["users", "user", "User", "profiles", "profile"],
              "activity_types": ["activity_types", "activitytypes", "ActivityType", "activity_type"],
              "focus_areas": ["focus_areas", "focusareas", "FocusArea", "focus_area"],
              "time_entries": ["time_entries", "timeentries", "TimeEntry", "time_entry", "timesheet_entries", "timesheets"],
              "timesheets": ["timesheets", "timesheet", "Timesheet"],
              "timesheet_entries": ["timesheet_entries", "timesheetentries", "TimesheetEntry", "timesheet_entry"],
              "clients": ["clients", "client", "Client"],
              "teams": ["teams", "team", "Team"],
              "departments": ["departments", "department", "Department"],
              "jobs": ["jobs", "job", "Job"],
              "devices": ["devices", "device", "Device"],
              "documents": ["documents", "document", "Document"],
              "locations": ["locations", "location", "Location"],
              "audit_logs": ["audit_logs", "auditlogs", "AuditLog", "audit_log"]
          }
          
          # Find actual table names
          tables = {}
          for logical_name, possible_names in table_mapping.items():
              for possible_name in possible_names:
                  if possible_name in available_tables:
                      tables[logical_name] = possible_name
                      print(f"‚úÖ Found table: {logical_name} -> {possible_name}")
                      break
              else:
                  print(f"‚ùå No table found for: {logical_name}")
          
          print(f"üìã Final table mapping: {tables}")
          
          # Export data for each table (only if table exists)
          results = {}
          
          # Export active companies
          if "companies" in tables:
              companies = fetch_data(tables["companies"], {"deletedAt": "is.null"})
              results["companies"] = len(companies) if not companies.empty else 0
              if not companies.empty:
                  companies.to_csv("exports/companies.csv", index=False)
          else:
              results["companies"] = 0
              
          # Export active projects
          if "projects" in tables:
              projects = fetch_data(tables["projects"], {"deletedAt": "is.null"})
              results["projects"] = len(projects) if not projects.empty else 0
              if not projects.empty:
                  projects.to_csv("exports/projects.csv", index=False)
          else:
              results["projects"] = 0
              
          # Export active users
          if "users" in tables:
              users = fetch_data(tables["users"], {"deletedAt": "is.null"})
              results["users"] = len(users) if not users.empty else 0
              if not users.empty:
                  # Remove sensitive fields
                  sensitive_fields = ["password", "password_hash", "encrypted_password", "auth_token", "api_key", "remember_token"]
                  for field in sensitive_fields:
                      if field in users.columns:
                          users = users.drop(columns=[field])
                  users.to_csv("exports/users.csv", index=False)
          else:
              results["users"] = 0
              
          # Export activity types
          if "activity_types" in tables:
              activity_types = fetch_data(tables["activity_types"], {"deletedAt": "is.null"})
              results["activity_types"] = len(activity_types) if not activity_types.empty else 0
              if not activity_types.empty:
                  activity_types.to_csv("exports/activity_types.csv", index=False)
          else:
              results["activity_types"] = 0
              
          # Export focus areas
          if "focus_areas" in tables:
              focus_areas = fetch_data(tables["focus_areas"], {"deletedAt": "is.null"})
              results["focus_areas"] = len(focus_areas) if not focus_areas.empty else 0
              if not focus_areas.empty:
                  focus_areas.to_csv("exports/focus_areas.csv", index=False)
          else:
              results["focus_areas"] = 0
              
          # Export timesheets from last 30 days
          if "timesheets" in tables:
              timesheets = fetch_data(tables["timesheets"], 
                                    {"deletedAt": "is.null"})
              results["timesheets"] = len(timesheets) if not timesheets.empty else 0
              if not timesheets.empty:
                  timesheets.to_csv("exports/timesheets.csv", index=False)
          else:
              results["timesheets"] = 0
              
          # Export timesheet entries from last 30 days  
          if "timesheet_entries" in tables:
              timesheet_entries = fetch_data(tables["timesheet_entries"], 
                                            {"deletedAt": "is.null", 
                                             "created_at": f"gte.{thirty_days_ago}"})
              results["timesheet_entries"] = len(timesheet_entries) if not timesheet_entries.empty else 0
              if not timesheet_entries.empty:
                  timesheet_entries.to_csv("exports/timesheet_entries.csv", index=False)
          else:
              results["timesheet_entries"] = 0
              
          # Export time entries (fallback)
          if "time_entries" in tables:
              time_entries = fetch_data(tables["time_entries"], 
                                      {"deletedAt": "is.null", 
                                       "created_at": f"gte.{thirty_days_ago}"})
              results["time_entries"] = len(time_entries) if not time_entries.empty else 0
              if not time_entries.empty:
                  time_entries.to_csv("exports/time_entries.csv", index=False)
          else:
              results["time_entries"] = 0
              
          # Export clients
          if "clients" in tables:
              clients = fetch_data(tables["clients"], {"deletedAt": "is.null"})
              results["clients"] = len(clients) if not clients.empty else 0
              if not clients.empty:
                  clients.to_csv("exports/clients.csv", index=False)
          else:
              results["clients"] = 0
              
          # Export teams
          if "teams" in tables:
              teams = fetch_data(tables["teams"], {"deletedAt": "is.null"})
              results["teams"] = len(teams) if not teams.empty else 0
              if not teams.empty:
                  teams.to_csv("exports/teams.csv", index=False)
          else:
              results["teams"] = 0
              
          # Export departments
          if "departments" in tables:
              departments = fetch_data(tables["departments"], {"deletedAt": "is.null"})
              results["departments"] = len(departments) if not departments.empty else 0
              if not departments.empty:
                  departments.to_csv("exports/departments.csv", index=False)
          else:
              results["departments"] = 0
              
          # Create a summary file with export information
          with open("exports/export_summary.json", "w") as f:
              summary = {
                  "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                  "tables_exported": results,
                  "time_entries_period": f"{thirty_days_ago} to {current_date}"
              }
              json.dump(summary, f, indent=2)
          
          # Get current timestamp for HTML
          timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
          
          # Create an index.html file for GitHub Pages
          html_content = f"""<!DOCTYPE html>
          <html>
          <head>
              <title>Supabase Data Exports</title>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1">
              <style>
                  body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; line-height: 1.6; }}
                  h1 {{ color: #333; }}
                  .container {{ max-width: 800px; margin: 0 auto; }}
                  table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                  th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                  th {{ background-color: #f2f2f2; }}
                  .download-btn {{ display: inline-block; background-color: #4CAF50; color: white; 
                                  padding: 8px 16px; text-decoration: none; border-radius: 4px; }}
                  .download-btn:hover {{ background-color: #45a049; }}
                  .timestamp {{ color: #666; font-style: italic; }}
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>Supabase Data Exports</h1>
                  <p class="timestamp">Last updated: {timestamp} UTC</p>
                  
                  <table>
                      <tr>
                          <th>Dataset</th>
                          <th>Records</th>
                          <th>Download</th>
                      </tr>"""
          
          # Add table rows dynamically
          for table_name, count in results.items():
              file_name = f"{table_name}.csv"
              display_name = table_name.replace("_", " ").title()
              
              if table_name == "time_entries":
                  display_name += " (Last 30 Days)"
                  
              download_link = f"""
                      <tr>
                          <td>{display_name}</td>
                          <td>{count}</td>
                          <td>"""
              
              if count > 0:
                  download_link += f"""<a href="{file_name}" class="download-btn">Download CSV</a>"""
              else:
                  download_link += """<span style="color: #999;">No data</span>"""
                  
              download_link += """</td>
                      </tr>"""
              
              html_content += download_link
          
          # Complete the HTML
          html_content += f"""
                  </table>
                  
                  <h2>Export Summary</h2>
                  <p>Time period for time entries: {thirty_days_ago} to {current_date}</p>
                  <p><a href="export_summary.json">View detailed export summary (JSON)</a></p>
              </div>
          </body>
          </html>"""
          
          with open("exports/index.html", "w") as f:
              f.write(html_content)
          
          # Check if we have any successful exports
          successful_exports = sum(1 for count in results.values() if count > 0)
          if successful_exports == 0:
              print("‚ö†Ô∏è Warning: No data was successfully exported from any table")
              print("Please check your Supabase configuration and table access permissions")
              
              # Create a placeholder file
              with open("exports/README.md", "w") as f:
                  f.write(f"""# Supabase Export Attempt
          
          No data could be exported on {timestamp}.
          
          Possible reasons:
          1. Table names might be different
          2. API key permissions
          3. Row Level Security (RLS) policies
          4. Tables in non-public schema
          
          Please check your Supabase configuration.
          """)
          else:
              print(f"‚úÖ Successfully exported data from {successful_exports} tables")
          '

      - name: Check for exported files
        run: |
          echo "üìÅ Contents of exports directory:"
          ls -la exports/ || echo "No exports directory found"
          
          if [ -d "exports" ] && [ "$(ls -A exports)" ]; then
            echo "‚úÖ Export files found"
          else
            echo "‚ö†Ô∏è No export files found, creating placeholder"
            mkdir -p exports
            echo "No data available" > exports/no-data.txt
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: supabase-exports
          path: exports/
          retention-days: 30

      # Only deploy to GitHub Pages if we're on main/master branch and files exist
      - name: Setup Pages
        uses: actions/configure-pages@v4
        if: (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && github.event_name != 'schedule'

      - name: Upload to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        if: (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && github.event_name != 'schedule'
        with:
          path: './exports'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
        if: (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && github.event_name != 'schedule'

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add exports/
          git diff --staged --quiet || git commit -m "Update exported data - $(date)"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
