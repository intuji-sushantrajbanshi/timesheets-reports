name: Export Supabase Data to CSV

on:
  schedule:
    - cron: '15 11 * * *' # Every day at 5 PM Nepal/Kathmandu Time (UTC+5:45)
  workflow_dispatch:
  push:
    branches:
      - master
      - main

permissions:
  contents: write

jobs:
  export-data:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas python-dateutil pytz

      - name: Check Supabase credentials
        run: |
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "âŒ Supabase credentials are not properly set in GitHub secrets."
            exit 1
          else
            echo "âœ… Supabase credentials found"
          fi

      - name: Create Python export script
        run: |
          cat > export_script.py << 'EOF'
          import os
          import requests
          import pandas as pd
          import json
          from datetime import datetime, date
          import sys
          
          def main():
              try:
                  # Configuration
                  supabase_url = os.environ["SUPABASE_URL"]
                  supabase_key = os.environ["SUPABASE_KEY"]
                  
                  # Common headers for all requests
                  headers = {
                      "apikey": supabase_key,
                      "Authorization": f"Bearer {supabase_key}",
                      "Content-Type": "application/json",
                      "Prefer": "return=representation"
                  }
                  
                  def fetch_data(table_name, query_params=None, select_fields="*"):
                      endpoint = f"{supabase_url}/rest/v1/{table_name}"
                      
                      params = {}
                      if select_fields != "*":
                          params["select"] = select_fields
                          
                      if query_params:
                          params.update(query_params)
                          
                      try:
                          print(f"ðŸ” Fetching from {endpoint} with params {params}")
                          response = requests.get(endpoint, headers=headers, params=params)
                          
                          if response.status_code == 200:
                              data = response.json()
                              if not data:
                                  print(f"No data returned for {table_name}")
                                  return pd.DataFrame()
                                  
                              df = pd.DataFrame(data)
                              print(f"âœ… Successfully fetched {len(df)} rows from {table_name}")
                              return df
                          else:
                              print(f"âŒ Error fetching {table_name}: {response.status_code} - {response.text}")
                              return pd.DataFrame()
                              
                      except Exception as e:
                          print(f"âŒ Error fetching {table_name}: {str(e)}")
                          return pd.DataFrame()
                  
                  # Get company ID for "Timesheet App (Production)"
                  print("ðŸ” Fetching company ID for Timesheet App (Production)")
                  company_df = fetch_data("Company", {"name": "eq.Timesheet App (Production)"}, "id")
                  
                  if company_df.empty or "id" not in company_df.columns:
                      print("âŒ Company not found")
                      with open("exports/error.txt", "w") as f:
                          f.write("Company 'Timesheet App (Production)' not found")
                      sys.exit(1)
                      
                  company_id = company_df.iloc[0]["id"]
                  print(f"âœ… Company ID: {company_id}")
                  
                  # Configuration for the query (matching your SQL query structure)
                  # You can modify these values as needed
                  TARGET_PROJECT = "Department of Health (Government of Western Australia)"
                  DATE_FILTER = "TILL_DATE"  # Options: TILL_DATE, TODAY, THIS_WEEK, LAST_WEEK, THIS_MONTH, LAST_MONTH
                  
                  print(f"ðŸ“Š Extracting data for project: {TARGET_PROJECT}")
                  print(f"ðŸ“… Date filter: {DATE_FILTER}")
                  
                  # Calculate date range based on filter
                  today = date.today()
                  
                  if DATE_FILTER == "TODAY":
                      start_date = today
                      end_date = today
                  elif DATE_FILTER == "THIS_WEEK":
                      # Monday to Sunday
                      start_date = today - pd.Timedelta(days=today.weekday())
                      end_date = start_date + pd.Timedelta(days=6)
                  elif DATE_FILTER == "LAST_WEEK":
                      # Previous Monday to Sunday
                      start_date = today - pd.Timedelta(days=today.weekday() + 7)
                      end_date = start_date + pd.Timedelta(days=6)
                  elif DATE_FILTER == "THIS_MONTH":
                      start_date = today.replace(day=1)
                      if today.month == 12:
                          end_date = today.replace(year=today.year + 1, month=1, day=1) - pd.Timedelta(days=1)
                      else:
                          end_date = today.replace(month=today.month + 1, day=1) - pd.Timedelta(days=1)
                  elif DATE_FILTER == "LAST_MONTH":
                      if today.month == 1:
                          start_date = today.replace(year=today.year - 1, month=12, day=1)
                          end_date = today.replace(day=1) - pd.Timedelta(days=1)
                      else:
                          start_date = today.replace(month=today.month - 1, day=1)
                          end_date = today.replace(day=1) - pd.Timedelta(days=1)
                  else:  # TILL_DATE
                      start_date = date(1900, 1, 1)
                      end_date = date(2100, 12, 31)
                  
                  print(f"ðŸ“… Date range: {start_date} to {end_date}")
                  
                  # Fetch all required data
                  print("ðŸ“Š Fetching time entries...")
                  time_entries_df = fetch_data(
                      "TimeEntry",
                      {
                          "companyId": f"eq.{company_id}",
                          "deletedAt": "is.null",
                          "entryDate": f"gte.{start_date}",
                          "entryDate": f"lte.{end_date}"
                      }
                  )
                  
                  print("ðŸ“Š Fetching projects...")  
                  projects_df = fetch_data(
                      "Project",
                      {
                          "companyId": f"eq.{company_id}",
                          "deletedAt": "is.null",
                          "title": f"eq.{TARGET_PROJECT}"
                      }
                  )
                  
                  print("ðŸ“Š Fetching users...")
                  users_df = fetch_data(
                      "User",
                      {
                          "companyId": f"eq.{company_id}",
                          "deletedAt": "is.null"
                      }
                  )
                  
                  print("ðŸ“Š Fetching focus areas...")
                  focus_areas_df = fetch_data(
                      "FocusArea",
                      {
                          "companyId": f"eq.{company_id}",
                          "deletedAt": "is.null"
                      }
                  )
                  
                  print("ðŸ“Š Fetching activity types...")
                  activity_types_df = fetch_data(
                      "ActivityType",
                      {
                          "companyId": f"eq.{company_id}",
                          "deletedAt": "is.null"
                      }
                  )
                  
                  # Check if we have all required data
                  if time_entries_df.empty:
                      print("âŒ No time entries found")
                      return False
                  if projects_df.empty:
                      print("âŒ Target project not found")
                      return False
                  if users_df.empty:
                      print("âŒ No users found")
                      return False
                  if focus_areas_df.empty:
                      print("âŒ No focus areas found")
                      return False
                  if activity_types_df.empty:
                      print("âŒ No activity types found")
                      return False
                  
                  print(f"âœ… Data validation successful")
                  print(f"   - Time entries: {len(time_entries_df)}")
                  print(f"   - Projects: {len(projects_df)}")
                  print(f"   - Users: {len(users_df)}")
                  print(f"   - Focus areas: {len(focus_areas_df)}")
                  print(f"   - Activity types: {len(activity_types_df)}")
                  
                  # Get the project ID for filtering
                  project_id = projects_df.iloc[0]["id"]
                  
                  # Filter time entries for the target project
                  filtered_time_entries = time_entries_df[
                      time_entries_df["projectId"] == project_id
                  ]
                  
                  if filtered_time_entries.empty:
                      print("âŒ No time entries found for the target project")
                      return False
                  
                  print(f"âœ… Found {len(filtered_time_entries)} time entries for target project")
                  
                  # Merge all data together
                  merged_df = filtered_time_entries.merge(
                      projects_df[["id", "title"]], 
                      left_on="projectId", 
                      right_on="id", 
                      how="inner",
                      suffixes=("", "_project")
                  )
                  
                  merged_df = merged_df.merge(
                      users_df[["id", "firstName", "lastName", "email"]], 
                      left_on="userId", 
                      right_on="id", 
                      how="inner",
                      suffixes=("", "_user")
                  )
                  
                  merged_df = merged_df.merge(
                      focus_areas_df[["id", "title", "colourCode"]], 
                      left_on="focusAreaId", 
                      right_on="id", 
                      how="inner",
                      suffixes=("", "_focus")
                  )
                  
                  merged_df = merged_df.merge(
                      activity_types_df[["id", "title"]], 
                      left_on="activityTypeId", 
                      right_on="id", 
                      how="inner",
                      suffixes=("", "_activity")
                  )
                  
                  print(f"âœ… Successfully merged data: {len(merged_df)} records")
                  
                  # Process the data to match your SQL query output
                  merged_df["user_name"] = (
                      merged_df["firstName"].fillna("").astype(str) + " " + 
                      merged_df["lastName"].fillna("").astype(str)
                  ).str.strip()
                  
                  # Calculate duration in minutes from start and end times
                  merged_df["startTime"] = pd.to_datetime(merged_df["startTime"])
                  merged_df["endTime"] = pd.to_datetime(merged_df["endTime"])
                  merged_df["duration_minutes"] = (
                      merged_df["endTime"] - merged_df["startTime"]
                  ).dt.total_seconds() / 60
                  
                  # Group by user, focus area, and activity type
                  grouped = merged_df.groupby([
                      "title",  # project name
                      "user_name",
                      "email",
                      "title_focus",  # focus area
                      "colourCode",  # focus area color
                      "title_activity"  # activity type
                  ]).agg({
                      "duration": "sum",  # total duration raw
                      "duration_minutes": "sum",  # calculated duration
                      "id": "count",  # total entries
                      "entryDate": ["min", "max"]  # date range
                  }).reset_index()
                  
                  # Flatten column names
                  grouped.columns = [
                      "project_name",
                      "user_name", 
                      "user_email",
                      "focus_area",
                      "focus_area_color",
                      "activity_type",
                      "total_duration_raw",
                      "total_duration_minutes",
                      "total_entries",
                      "first_entry_date",
                      "last_entry_date"
                  ]
                  
                  # Add filter information
                  grouped["applied_date_filter"] = DATE_FILTER
                  grouped["filter_start_date"] = start_date
                  grouped["filter_end_date"] = end_date
                  
                  # Round total hours to 2 decimal places
                  grouped["total_hours"] = grouped["total_duration_minutes"].round(2)
                  
                  # Reorder columns to match SQL query output
                  final_columns = [
                      "applied_date_filter",
                      "filter_start_date", 
                      "filter_end_date",
                      "project_name",
                      "user_name",
                      "user_email", 
                      "focus_area",
                      "focus_area_color",
                      "activity_type",
                      "total_duration_raw",
                      "total_hours",
                      "total_entries",
                      "first_entry_date",
                      "last_entry_date"
                  ]
                  
                  final_df = grouped[final_columns]
                  
                  # Sort by user name, focus area, and total hours (descending)
                  final_df = final_df.sort_values([
                      "user_name",
                      "focus_area", 
                      "total_hours"
                  ], ascending=[True, True, False])
                  
                  # Create exports directory
                  os.makedirs("exports", exist_ok=True)
                  
                  # Save to CSV
                  csv_filename = f"exports/project_time_report_{TARGET_PROJECT.replace(' ', '_').replace('(', '').replace(')', '')}_{DATE_FILTER}.csv"
                  final_df.to_csv(csv_filename, index=False)
                  print(f"âœ… Created project time report: {csv_filename}")
                  print(f"   Total records: {len(final_df)}")
                  
                  # Create summary
                  summary = {
                      "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                      "company_id": company_id,
                      "target_project": TARGET_PROJECT,
                      "date_filter": DATE_FILTER,
                      "date_range": f"{start_date} to {end_date}",
                      "total_records": len(final_df),
                      "csv_filename": csv_filename
                  }
                  
                  with open("exports/export_summary.json", "w") as f:
                      json.dump(summary, f, indent=2)
                  
                  print("âœ… Export completed successfully!")
                  return True
                      
              except Exception as e:
                  print(f"âŒ Error during export: {str(e)}")
                  import traceback
                  traceback.print_exc()
                  return False
          
          if __name__ == "__main__":
              success = main()
              if not success:
                  sys.exit(1)
          EOF

      - name: Export project time data from Supabase
        run: |
          mkdir -p exports
          python export_script.py

      - name: Check for exported files
        run: |
          echo "ðŸ“ Contents of exports directory:"
          ls -la exports/ || echo "No exports directory found"
          
          if [ -d "exports" ] && [ "$(ls -A exports)" ]; then
            echo "âœ… Export files found"
          else
            echo "âš ï¸ No export files found, creating placeholder"
            mkdir -p exports
            echo "No data available" > exports/no-data.txt
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: supabase-project-time-export
          path: exports/
          retention-days: 30

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add exports/
          git diff --staged --quiet || git commit -m "Update project time export - $(date)"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
