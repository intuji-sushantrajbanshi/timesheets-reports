name: Export Supabase Data to CSV

on:
  schedule:
    - cron: '15 11 * * *' # Every day at 5 PM Nepal/Kathmandu Time (UTC+5:45)
  workflow_dispatch: # Allow manual trigger
  push:
    branches:
      - master
      - main

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  export-data:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas python-dateutil pytz

      - name: Check Supabase credentials
        run: |
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "‚ùå Supabase credentials are not properly set in GitHub secrets."
            exit 1
          else
            echo "‚úÖ Supabase credentials found"
          fi

      - name: List available tables
        run: |
          python -c '
          import os
          import requests
          import json
          
          supabase_url = os.environ["SUPABASE_URL"]
          supabase_key = os.environ["SUPABASE_KEY"]
          
          headers = {
              "apikey": supabase_key,
              "Authorization": f"Bearer {supabase_key}",
              "Content-Type": "application/json"
          }
          
          # Try to get schema information
          try:
              response = requests.get(
                  f"{supabase_url}/rest/v1/",
                  headers=headers
              )
              
              print(f"Status code: {response.status_code}")
              print(f"Response: {response.text}")
              
              if response.status_code == 200:
                  print("‚úÖ Successfully connected to Supabase API")
              else:
                  print(f"‚ùå Error connecting to Supabase API: {response.status_code}")
          except Exception as e:
              print(f"‚ùå Error: {str(e)}")
          '

      - name: Export data from Supabase
        run: |
          mkdir -p exports
          python -c '
          import os
          import requests
          import pandas as pd
          import json
          from datetime import datetime, timedelta
          import pytz
          
          # Configuration
          supabase_url = os.environ["SUPABASE_URL"]
          supabase_key = os.environ["SUPABASE_KEY"]
          
          # Common headers for all requests
          headers = {
              "apikey": supabase_key,
              "Authorization": f"Bearer {supabase_key}",
              "Content-Type": "application/json",
              "Prefer": "return=representation"
          }
          
          def fetch_data(table_name, query_params=None, select_fields="*"):
              """Fetch data from Supabase table and return as DataFrame"""
              endpoint = f"{supabase_url}/rest/v1/{table_name}"
              
              params = {}
              if select_fields != "*":
                  params["select"] = select_fields
                  
              if query_params:
                  params.update(query_params)
                  
              try:
                  print(f"üîç Fetching from {endpoint} with params {params}")
                  response = requests.get(endpoint, headers=headers, params=params)
                  
                  if response.status_code == 200:
                      data = response.json()
                      if not data:
                          print(f"No data returned for {table_name}")
                          return pd.DataFrame()
                          
                      df = pd.DataFrame(data)
                      print(f"‚úÖ Successfully fetched {len(df)} rows from {table_name}")
                      return df
                  else:
                      print(f"‚ùå Error fetching {table_name}: {response.status_code} - {response.text}")
                      return pd.DataFrame()
                      
              except Exception as e:
                  print(f"‚ùå Error fetching {table_name}: {str(e)}")
                  return pd.DataFrame()
          
          # Calculate date for filtering time entries (last 30 days)
          thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
          current_date = datetime.now().strftime("%Y-%m-%d")
          
          # Define tables with lowercase names (PostgreSQL often uses lowercase)
          tables = {
              "companies": "company",
              "projects": "project",
              "users": "user",
              "activity_types": "activitytype",
              "focus_areas": "focusarea",
              "time_entries": "timeentry",
              "submissions": "submission"
          }
          
          # Export data for each table
          results = {}
          
          # Export active companies
          companies = fetch_data(tables["companies"], {"deletedAt": "is.null"})
          results["companies"] = len(companies) if not companies.empty else 0
          if not companies.empty:
              companies.to_csv("exports/companies.csv", index=False)
              
          # Export active projects
          projects = fetch_data(tables["projects"], {"deletedAt": "is.null"})
          results["projects"] = len(projects) if not projects.empty else 0
          if not projects.empty:
              projects.to_csv("exports/projects.csv", index=False)
              
          # Export active users
          users = fetch_data(tables["users"], {"deletedAt": "is.null"})
          results["users"] = len(users) if not users.empty else 0
          if not users.empty:
              # Remove sensitive fields
              if "password" in users.columns:
                  users = users.drop(columns=["password"])
              users.to_csv("exports/users.csv", index=False)
              
          # Export activity types
          activity_types = fetch_data(tables["activity_types"], {"deletedAt": "is.null"})
          results["activity_types"] = len(activity_types) if not activity_types.empty else 0
          if not activity_types.empty:
              activity_types.to_csv("exports/activity_types.csv", index=False)
              
          # Export focus areas
          focus_areas = fetch_data(tables["focus_areas"], {"deletedAt": "is.null"})
          results["focus_areas"] = len(focus_areas) if not focus_areas.empty else 0
          if not focus_areas.empty:
              focus_areas.to_csv("exports/focus_areas.csv", index=False)
              
          # Export time entries from last 30 days
          time_entries = fetch_data(tables["time_entries"], 
                                   {"deletedAt": "is.null", 
                                    "entryDate": f"gte.{thirty_days_ago}"})
          results["time_entries"] = len(time_entries) if not time_entries.empty else 0
          if not time_entries.empty:
              time_entries.to_csv("exports/time_entries.csv", index=False)
              
          # Export submissions
          submissions = fetch_data(tables["submissions"], {"deletedAt": "is.null"})
          results["submissions"] = len(submissions) if not submissions.empty else 0
          if not submissions.empty:
              submissions.to_csv("exports/submissions.csv", index=False)
              
          # Create a summary file with export information
          with open("exports/export_summary.json", "w") as f:
              summary = {
                  "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                  "tables_exported": results,
                  "time_entries_period": f"{thirty_days_ago} to {current_date}"
              }
              json.dump(summary, f, indent=2)
          
          # Get current timestamp for HTML
          timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
          
          # Create an index.html file for GitHub Pages
          html_content = f"""<!DOCTYPE html>
          <html>
          <head>
              <title>Supabase Data Exports</title>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1">
              <style>
                  body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; line-height: 1.6; }}
                  h1 {{ color: #333; }}
                  .container {{ max-width: 800px; margin: 0 auto; }}
                  table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                  th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                  th {{ background-color: #f2f2f2; }}
                  .download-btn {{ display: inline-block; background-color: #4CAF50; color: white; 
                                  padding: 8px 16px; text-decoration: none; border-radius: 4px; }}
                  .download-btn:hover {{ background-color: #45a049; }}
                  .timestamp {{ color: #666; font-style: italic; }}
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>Supabase Data Exports</h1>
                  <p class="timestamp">Last updated: {timestamp} UTC</p>
                  
                  <table>
                      <tr>
                          <th>Dataset</th>
                          <th>Records</th>
                          <th>Download</th>
                      </tr>"""
          
          # Add table rows dynamically based on what was successfully exported
          for table_name, count in results.items():
              file_name = f"{table_name}.csv"
              display_name = table_name.replace("_", " ").title()
              
              if table_name == "time_entries":
                  display_name += " (Last 30 Days)"
                  
              download_link = f"""
                      <tr>
                          <td>{display_name}</td>
                          <td>{count}</td>
                          <td>"""
              
              if count > 0:
                  download_link += f"""<a href="{file_name}" class="download-btn">Download CSV</a>"""
              else:
                  download_link += """<span style="color: #999;">No data</span>"""
                  
              download_link += """</td>
                      </tr>"""
              
              html_content += download_link
          
          # Complete the HTML
          html_content += f"""
                  </table>
                  
                  <h2>Export Summary</h2>
                  <p>Time period for time entries: {thirty_days_ago} to {current_date}</p>
                  <p><a href="export_summary.json">View detailed export summary (JSON)</a></p>
              </div>
          </body>
          </html>"""
          
          with open("exports/index.html", "w") as f:
              f.write(html_content)
          
          # Check if we have any successful exports
          successful_exports = sum(1 for count in results.values() if count > 0)
          if successful_exports == 0:
              print("‚ö†Ô∏è Warning: No data was successfully exported from any table")
              print("Please check your Supabase configuration and table access permissions")
          else:
              print(f"‚úÖ Successfully exported data from {successful_exports} tables")
          '

      - name: Check for exported files
        id: check_exports
        run: |
          if [ -f exports/export_summary.json ]; then
            echo "Exports summary file exists"
            cat exports/export_summary.json
            echo "export_exists=true" >> $GITHUB_OUTPUT
          else
            echo "No exports were generated"
            echo "export_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload CSV artifacts
        if: steps.check_exports.outputs.export_exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: supabase-data-exports-${{ github.run_number }}
          path: exports/
          retention-days: 30

      - name: Deploy to GitHub Pages
        if: steps.check_exports.outputs.export_exists == 'true'
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: exports
          publish_branch: gh-pages
          keep_files: false
          commit_message: "üìä Update Supabase data exports - ${{ github.run_number }}"
