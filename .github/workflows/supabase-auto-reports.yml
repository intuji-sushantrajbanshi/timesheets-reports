name: Export Supabase Data to CSV

on:
  schedule:
    - cron: '15 11 * * *' # Every day at 5 PM Nepal/Kathmandu Time (UTC+5:45)
  workflow_dispatch:
  push:
    branches:
      - master
      - main

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  export-data:
    runs-on: ubuntu-latest
    # Remove environment block to avoid protection rules
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas python-dateutil pytz

      - name: Check Supabase credentials
        run: |
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "‚ùå Supabase credentials are not properly set in GitHub secrets."
            exit 1
          else
            echo "‚úÖ Supabase credentials found"
          fi

      - name: Export data from Supabase
        run: |
          mkdir -p exports
          python -c '
          import os
          import requests
          import pandas as pd
          import json
          from datetime import datetime, timedelta
          
          # Configuration
          supabase_url = os.environ["SUPABASE_URL"]
          supabase_key = os.environ["SUPABASE_KEY"]
          
          # Common headers for all requests
          headers = {
              "apikey": supabase_key,
              "Authorization": f"Bearer {supabase_key}",
              "Content-Type": "application/json",
              "Prefer": "return=representation"
          }
          
          def fetch_data(table_name, query_params=None, select_fields="*"):
              endpoint = f"{supabase_url}/rest/v1/{table_name}"
              
              params = {}
              if select_fields != "*":
                  params["select"] = select_fields
                  
              if query_params:
                  params.update(query_params)
                  
              try:
                  print(f"üîç Fetching from {endpoint} with params {params}")
                  response = requests.get(endpoint, headers=headers, params=params)
                  
                  if response.status_code == 200:
                      data = response.json()
                      if not data:
                          print(f"No data returned for {table_name}")
                          return pd.DataFrame()
                          
                      df = pd.DataFrame(data)
                      print(f"‚úÖ Successfully fetched {len(df)} rows from {table_name}")
                      return df
                  else:
                      print(f"‚ùå Error fetching {table_name}: {response.status_code} - {response.text}")
                      return pd.DataFrame()
                      
              except Exception as e:
                  print(f"‚ùå Error fetching {table_name}: {str(e)}")
                  return pd.DataFrame()
          
          # Calculate date for filtering time entries (last 30 days)
          thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
          current_date = datetime.now().strftime("%Y-%m-%d")
          
          # Function to get all available tables
          def get_available_tables():
              """Get list of all available tables from Supabase"""
              try:
                  # Try to get schema information
                  response = requests.get(f"{supabase_url}/rest/v1/", headers=headers)
                  if response.status_code == 200:
                      # Parse the OpenAPI spec to get table names
                      openapi_data = response.json()
                      if "paths" in openapi_data:
                          tables = []
                          for path in openapi_data["paths"].keys():
                              if path.startswith("/") and not path.startswith("/rpc/"):
                                  table_name = path.strip("/")
                                  if table_name and "." not in table_name:
                                      tables.append(table_name)
                          return tables
              except Exception as e:
                  print(f"Could not fetch table list: {e}")
              
              # Fallback: common lowercase table names
              return ["companies", "projects", "users", "activity_types", "focus_areas", "time_entries", "submissions"]
          
          # Get available tables
          available_tables = get_available_tables()
          print(f"üîç Available tables: {available_tables}")
          
          # Map common table names to what might exist in your database
          table_mapping = {
              "companies": ["companies", "company", "Company"],
              "projects": ["projects", "project", "Project"], 
              "users": ["users", "user", "User", "profiles", "profile"],
              "activity_types": ["activity_types", "activitytypes", "ActivityType", "activity_type"],
              "focus_areas": ["focus_areas", "focusareas", "FocusArea", "focus_area"],
              "time_entries": ["time_entries", "timeentries", "TimeEntry", "time_entry", "timesheet_entries", "timesheets"],
              "timesheets": ["timesheets", "timesheet", "Timesheet"],
              "timesheet_entries": ["timesheet_entries", "timesheetentries", "TimesheetEntry", "timesheet_entry"],
              "clients": ["clients", "client", "Client"],
              "teams": ["teams", "team", "Team"],
              "departments": ["departments", "department", "Department"],
              "jobs": ["jobs", "job", "Job"],
              "devices": ["devices", "device", "Device"],
              "documents": ["documents", "document", "Document"],
              "locations": ["locations", "location", "Location"],
              "audit_logs": ["audit_logs", "auditlogs", "AuditLog", "audit_log"]
          }
          
          # Find actual table names
          tables = {}
          for logical_name, possible_names in table_mapping.items():
              for possible_name in possible_names:
                  if possible_name in available_tables:
                      tables[logical_name] = possible_name
                      print(f"‚úÖ Found table: {logical_name} -> {possible_name}")
                      break
              else:
                  print(f"‚ùå No table found for: {logical_name}")
          
          print(f"üìã Final table mapping: {tables}")
          
          # Export data for each table (only if table exists)
          results = {}
          
          # Export active companies
          if "companies" in tables:
              companies = fetch_data(tables["companies"], {"deletedAt": "is.null"})
              results["companies"] = len(companies) if not companies.empty else 0
              if not companies.empty:
                  companies.to_csv("exports/companies.csv", index=False)
          else:
              results["companies"] = 0
              
          # Export active projects
          if "projects" in tables:
              projects = fetch_data(tables["projects"], {"deletedAt": "is.null"})
              results["projects"] = len(projects) if not projects.empty else 0
              if not projects.empty:
                  projects.to_csv("exports/projects.csv", index=False)
          else:
              results["projects"] = 0
              
          # Export active users
          if "users" in tables:
              users = fetch_data(tables["users"], {"deletedAt": "is.null"})
              results["users"] = len(users) if not users.empty else 0
              if not users.empty:
                  # Remove sensitive fields
                  sensitive_fields = ["password", "password_hash", "encrypted_password", "auth_token", "api_key", "remember_token"]
                  for field in sensitive_fields:
                      if field in users.columns:
                          users = users.drop(columns=[field])
                  users.to_csv("exports/users.csv", index=False)
          else:
              results["users"] = 0
              
          # Export activity types
          if "activity_types" in tables:
              activity_types = fetch_data(tables["activity_types"], {"deletedAt": "is.null"})
              results["activity_types"] = len(activity_types) if not activity_types.empty else 0
              if not activity_types.empty:
                  activity_types.to_csv("exports/activity_types.csv", index=False)
          else:
              results["activity_types"] = 0
              
          # Export focus areas
          if "focus_areas" in tables:
              focus_areas = fetch_data(tables["focus_areas"], {"deletedAt": "is.null"})
              results["focus_areas"] = len(focus_areas) if not focus_areas.empty else 0
              if not focus_areas.empty:
                  focus_areas.to_csv("exports/focus_areas.csv", index=False)
          else:
              results["focus_areas"] = 0
              
          # Export timesheets from last 30 days
          if "timesheets" in tables:
              timesheets = fetch_data(tables["timesheets"], 
                                    {"deletedAt": "is.null"})
              results["timesheets"] = len(timesheets) if not timesheets.empty else 0
              if not timesheets.empty:
                  timesheets.to_csv("exports/timesheets.csv", index=False)
          else:
              results["timesheets"] = 0
              
          # Export timesheet entries from last 30 days  
          if "timesheet_entries" in tables:
              timesheet_entries = fetch_data(tables["timesheet_entries"], 
                                            {"deletedAt": "is.null", 
                                             "created_at": f"gte.{thirty_days_ago}"})
              results["timesheet_entries"] = len(timesheet_entries) if not timesheet_entries.empty else 0
              if not timesheet_entries.empty:
                  timesheet_entries.to_csv("exports/timesheet_entries.csv", index=False)
          else:
              results["timesheet_entries"] = 0
              
          # Export time entries (fallback)
          if "time_entries" in tables:
              time_entries = fetch_data(tables["time_entries"], 
                                      {"deletedAt": "is.null", 
                                       "created_at": f"gte.{thirty_days_ago}"})
              results["time_entries"] = len(time_entries) if not time_entries.empty else 0
              if not time_entries.empty:
                  time_entries.to_csv("exports/time_entries.csv", index=False)
          else:
              results["time_entries"] = 0
              
          # Export clients
          if "clients" in tables:
              clients = fetch_data(tables["clients"], {"deletedAt": "is.null"})
              results["clients"] = len(clients) if not clients.empty else 0
              if not clients.empty:
                  clients.to_csv("exports/clients.csv", index=False)
          else:
              results["clients"] = 0
              
          # Export teams
          if "teams" in tables:
              teams = fetch_data(tables["teams"], {"deletedAt": "is.null"})
              results["teams"] = len(teams) if not teams.empty else 0
              if not teams.empty:
                  teams.to_csv("exports/teams.csv", index=False)
          else:
              results["teams"] = 0
              
          # Export departments
          if "departments" in tables:
              departments = fetch_data(tables["departments"], {"deletedAt": "is.null"})
              results["departments"] = len(departments) if not departments.empty else 0
              if not departments.empty:
                  departments.to_csv("exports/departments.csv", index=False)
          else:
              results["departments"] = 0
              
          # Create a summary file with export information
          with open("exports/export_summary.json", "w") as f:
              summary = {
                  "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                  "tables_exported": results,
                  "time_entries_period": f"{thirty_days_ago} to {current_date}"
              }
              json.dump(summary, f, indent=2)
          
          # Get current timestamp for HTML
          timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
          
          # Create an index.html file for GitHub Pages
          html_content = f"""<!DOCTYPE html>
          <html>
          <head>
              <title>Supabase Data Exports</title>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1">
              <style>
                  body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; line-height: 1.6; }}
                  h1 {{ color: #333; }}
                  .container {{ max-width: 800px; margin: 0 auto; }}
                  table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                  th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                  th {{ background-color: #f2f2f2; }}
                  .download-btn {{ display: inline-block; background-color: #4CAF50; color: white; 
                                  padding: 8px 16px; text-decoration: none; border-radius: 4px; }}
                  .download-btn:hover {{ background-color: #45a049; }}
                  .timestamp {{ color: #666; font-style: italic; }}
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>Supabase Data Exports</h1>
                  <p class="timestamp">Last updated: {timestamp} UTC</p>
                  
                  <table>
                      <tr>
                          <th>Dataset</th>
                          <th>Records</th>
                          <th>Download</th>
                      </tr>"""
          
          # Add table rows dynamically
          for table_name, count in results.items():
              file_name = f"{table_name}.csv"
              display_name = table_name.replace("_", " ").title()
              
              if table_name == "time_entries":
                  display_name += " (Last 30 Days)"
              elif table_name == "project_time_report":
                  display_name = "üìä Project Time Report (Consolidated)"
                  
              download_link = f"""
                      <tr>
                          <td>{display_name}</td>
                          <td>{count}</td>
                          <td>"""
              
              if count > 0:
                  download_link += f"""<a href="{file_name}" class="download-btn">Download CSV</a>"""
              else:
                  download_link += """<span style="color: #999;">No data</span>"""
                  
              download_link += """</td>
                      </tr>"""
              
              html_content += download_link
          
          # Complete the HTML
          html_content += f"""
                  </table>
                  
                  <h2>Export Summary</h2>
                  <p>Time period for time entries: {thirty_days_ago} to {current_date}</p>
                  <p><strong>üìä Project Time Report:</strong> Consolidated time tracking data organized by project names with user details, total hours, and entry counts.</p>
                  <p><a href="export_summary.json">View detailed export summary (JSON)</a></p>
              </div>
          </body>
          </html>"""
          
          with open("exports/index.html", "w") as f:
              f.write(html_content)
          
          # Create consolidated project time report
          def create_project_time_report():
              """Create a consolidated CSV report with time data arranged by project names"""
              try:
                  # First, get all projects
                  projects_df = pd.DataFrame()
                  if "projects" in tables:
                      projects_df = fetch_data(tables["projects"], {"deletedAt": "is.null"})
                  
                  # Get users data
                  users_df = pd.DataFrame()
                  if "users" in tables:
                      users_df = fetch_data(tables["users"], {"deletedAt": "is.null"})
                      # Remove sensitive fields from users
                      sensitive_fields = ["password", "password_hash", "encrypted_password", "auth_token", "api_key", "remember_token"]
                      for field in sensitive_fields:
                          if field in users_df.columns:
                              users_df = users_df.drop(columns=[field])
                  
                  # Get time entries data (try different possible table names)
                  time_entries_df = pd.DataFrame()
                  time_table_used = None
                  
                  # Try different time entry table names
                  for time_table_key in ["time_entries", "timesheet_entries", "timesheets"]:
                      if time_table_key in tables:
                          temp_df = fetch_data(tables[time_table_key], {"deletedAt": "is.null"})
                          if not temp_df.empty:
                              time_entries_df = temp_df
                              time_table_used = time_table_key
                              print(f"‚úÖ Using time data from: {tables[time_table_key]}")
                              break
                  
                  if projects_df.empty or users_df.empty or time_entries_df.empty:
                      print("‚ùå Missing required data for project time report")
                      print(f"Projects: {len(projects_df)}, Users: {len(users_df)}, Time entries: {len(time_entries_df)}")
                      return
                  
                  # Print column names for debugging
                  print(f"üìã Projects columns: {list(projects_df.columns)}")
                  print(f"üìã Users columns: {list(users_df.columns)}")
                  print(f"üìã Time entries columns: {list(time_entries_df.columns)}")
                  
                  # Try to identify the correct column names (case-insensitive matching)
                  def find_column(df, possible_names):
                      """Find column name from list of possibilities (case-insensitive)"""
                      df_columns = [col.lower() for col in df.columns]
                      for name in possible_names:
                          if name.lower() in df_columns:
                              # Return the actual column name with correct case
                              return df.columns[df_columns.index(name.lower())]
                      return None
                  
                  # Map column names
                  project_id_col = find_column(projects_df, ["id", "project_id", "projectId"])
                  project_name_col = find_column(projects_df, ["title", "name", "project_name", "projectName"])
                  
                  user_id_col = find_column(users_df, ["id", "user_id", "userId"])
                  user_first_name_col = find_column(users_df, ["firstName", "first_name", "fname"])
                  user_last_name_col = find_column(users_df, ["lastName", "last_name", "lname"])
                  user_email_col = find_column(users_df, ["email", "email_address"])
                  
                  time_project_id_col = find_column(time_entries_df, ["projectId", "project_id", "project"])
                  time_user_id_col = find_column(time_entries_df, ["userId", "user_id", "user"])
                  time_duration_col = find_column(time_entries_df, ["duration", "hours", "time_spent", "total_time"])
                  time_start_col = find_column(time_entries_df, ["startTime", "start_time", "started_at", "start"])
                  time_end_col = find_column(time_entries_df, ["endTime", "end_time", "ended_at", "end"])
                  time_date_col = find_column(time_entries_df, ["entryDate", "entry_date", "date", "created_at"])
                  
                  print(f"üîç Column mapping:")
                  print(f"   Project: id={project_id_col}, name={project_name_col}")
                  print(f"   User: id={user_id_col}, first={user_first_name_col}, last={user_last_name_col}, email={user_email_col}")
                  print(f"   Time: project_id={time_project_id_col}, user_id={time_user_id_col}, duration={time_duration_col}")
                  
                  # Check if we have the essential columns
                  if not all([project_id_col, project_name_col, user_id_col, time_project_id_col, time_user_id_col]):
                      print("‚ùå Cannot find essential columns for project time report")
                      return
                  
                  # Merge the data
                  # First merge time entries with projects
                  merged_df = time_entries_df.merge(
                      projects_df[[project_id_col, project_name_col]], 
                      left_on=time_project_id_col, 
                      right_on=project_id_col, 
                      how="inner"
                  )
                  
                  # Then merge with users
                  user_cols_to_merge = [user_id_col]
                  if user_first_name_col: user_cols_to_merge.append(user_first_name_col)
                  if user_last_name_col: user_cols_to_merge.append(user_last_name_col)
                  if user_email_col: user_cols_to_merge.append(user_email_col)
                  
                  merged_df = merged_df.merge(
                      users_df[user_cols_to_merge], 
                      left_on=time_user_id_col, 
                      right_on=user_id_col, 
                      how="inner"
                  )
                  
                  if merged_df.empty:
                      print("‚ùå No data after merging tables")
                      return
                  
                  print(f"‚úÖ Successfully merged data: {len(merged_df)} time entries")
                  
                  # Create user name column
                  if user_first_name_col and user_last_name_col:
                      merged_df["user_name"] = merged_df[user_first_name_col].fillna("") + " " + merged_df[user_last_name_col].fillna("")
                      merged_df["user_name"] = merged_df["user_name"].str.strip()
                  else:
                      merged_df["user_name"] = merged_df[user_email_col] if user_email_col else "Unknown User"
                  
                  # Calculate duration (try multiple approaches)
                  merged_df["calculated_duration"] = 0
                  
                  # Method 1: Use existing duration column (convert to minutes if needed)
                  if time_duration_col and time_duration_col in merged_df.columns:
                      duration_values = pd.to_numeric(merged_df[time_duration_col], errors="coerce").fillna(0)
                      # If values are very large, they might be in milliseconds or seconds
                      if duration_values.max() > 1440:  # More than 24 hours in minutes
                          if duration_values.max() > 86400:  # More than 24 hours in seconds
                              duration_values = duration_values / 60  # Convert seconds to minutes
                          else:
                              duration_values = duration_values  # Already in minutes
                      merged_df["calculated_duration"] = duration_values
                  
                  # Method 2: Calculate from start/end times
                  elif time_start_col and time_end_col:
                      try:
                          merged_df[time_start_col] = pd.to_datetime(merged_df[time_start_col], errors="coerce")
                          merged_df[time_end_col] = pd.to_datetime(merged_df[time_end_col], errors="coerce")
                          time_diff = (merged_df[time_end_col] - merged_df[time_start_col]).dt.total_seconds() / 60
                          merged_df["calculated_duration"] = time_diff.fillna(0)
                      except Exception as e:
                          print(f"‚ö†Ô∏è Could not calculate duration from start/end times: {e}")
                  
                  # Group by project and user
                  grouped = merged_df.groupby([project_name_col, "user_name", user_email_col if user_email_col else "user_name"]).agg({
                      "calculated_duration": ["sum", "count"],
                      time_date_col: ["min", "max"] if time_date_col else ["count", "count"]
                  }).reset_index()
                  
                  # Flatten column names
                  grouped.columns = [f"{col[0]}_{col[1]}" if col[1] else col[0] for col in grouped.columns]
                  
                  # Rename columns for clarity
                  column_renames = {}
                  for col in grouped.columns:
                      if "calculated_duration_sum" in col:
                          column_renames[col] = "total_duration_minutes"
                      elif "calculated_duration_count" in col:
                          column_renames[col] = "total_entries"
                      elif time_date_col and f"{time_date_col}_min" in col:
                          column_renames[col] = "first_entry_date"
                      elif time_date_col and f"{time_date_col}_max" in col:
                          column_renames[col] = "last_entry_date"
                  
                  grouped = grouped.rename(columns=column_renames)
                  
                  # Calculate hours
                  if "total_duration_minutes" in grouped.columns:
                      grouped["total_hours"] = (grouped["total_duration_minutes"] / 60).round(2)
                  else:
                      grouped["total_hours"] = 0
                  
                  # Rename project column
                  project_col_name = [col for col in grouped.columns if project_name_col in col][0]
                  grouped = grouped.rename(columns={project_col_name: "project_name"})
                  
                  # Sort by project name and total hours (descending)
                  grouped = grouped.sort_values(["project_name", "total_hours"], ascending=[True, False])
                  
                  # Create final report with separators between projects
                  final_report = []
                  current_project = None
                  
                  for _, row in grouped.iterrows():
                      # Add separator when project changes
                      if current_project != row["project_name"]:
                          if current_project is not None:  # Not the first project
                              final_report.append({
                                  "project_name": "--- SEPARATOR ---",
                                  "user_name": "",
                                  "user_email": "",
                                  "total_duration_minutes": None,
                                  "total_hours": None,
                                  "total_entries": None,
                                  "first_entry_date": None,
                                  "last_entry_date": None
                              })
                          current_project = row["project_name"]
                      
                      # Add the actual data row
                      report_row = {
                          "project_name": row["project_name"],
                          "user_name": row["user_name"],
                          "user_email": row.get(user_email_col + "_", row["user_name"]) if user_email_col else row["user_name"],
                          "total_duration_minutes": row.get("total_duration_minutes", 0),
                          "total_hours": row.get("total_hours", 0),
                          "total_entries": row.get("total_entries", 0),
                          "first_entry_date": row.get("first_entry_date", ""),
                          "last_entry_date": row.get("last_entry_date", "")
                      }
                      final_report.append(report_row)
                  
                  # Convert to DataFrame and save
                  if final_report:
                      report_df = pd.DataFrame(final_report)
                      report_df.to_csv("exports/project_time_report.csv", index=False)
                      print(f"‚úÖ Created project time report with {len(report_df)} rows")
                      results["project_time_report"] = len([row for row in final_report if row["project_name"] != "--- SEPARATOR ---"])
                  else:
                      print("‚ùå No data for project time report")
                      results["project_time_report"] = 0
                      
              except Exception as e:
                  print(f"‚ùå Error creating project time report: {str(e)}")
                  import traceback
                  traceback.print_exc()
                  results["project_time_report"] = 0
          
          # Generate the project time report
          create_project_time_report()
          
          # Check if we have any successful exports
          successful_exports = sum(1 for count in results.values() if count > 0)
          if successful_exports == 0:
              print("‚ö†Ô∏è Warning: No data was successfully exported from any table")
              print("Please check your Supabase configuration and table access permissions")
              
              # Create a placeholder file
              with open("exports/README.md", "w") as f:
                  f.write(f"""# Supabase Export Attempt
          
          No data could be exported on {timestamp}.
          
          Possible reasons:
          1. Table names might be different
          2. API key permissions
          3. Row Level Security (RLS) policies
          4. Tables in non-public schema
          
          Please check your Supabase configuration.
          """)
          else:
              print(f"‚úÖ Successfully exported data from {successful_exports} tables")
          '

      - name: Check for exported files
        run: |
          echo "üìÅ Contents of exports directory:"
          ls -la exports/ || echo "No exports directory found"
          
          if [ -d "exports" ] && [ "$(ls -A exports)" ]; then
            echo "‚úÖ Export files found"
          else
            echo "‚ö†Ô∏è No export files found, creating placeholder"
            mkdir -p exports
            echo "No data available" > exports/no-data.txt
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: supabase-exports
          path: exports/
          retention-days: 30

      # Only deploy to GitHub Pages if we're on main/master branch and files exist
      - name: Setup Pages
        uses: actions/configure-pages@v4
        if: (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && github.event_name != 'schedule'

      - name: Upload to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        if: (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && github.event_name != 'schedule'
        with:
          path: './exports'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
        if: (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && github.event_name != 'schedule'

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add exports/
          git diff --staged --quiet || git commit -m "Update exported data - $(date)"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
