name: Export Supabase Data to CSV

on:
  schedule:
    - cron: '15 11 * * *' # Every day at 5 PM Nepal/Kathmandu Time (UTC+5:45)
  workflow_dispatch: # Allow manual trigger
  push:
    branches:
      - master
      - main

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  export-data:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas python-dateutil pytz

      - name: Check Supabase credentials
        run: |
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "‚ùå Supabase credentials are not properly set in GitHub secrets."
            exit 1
          else
            echo "‚úÖ Supabase credentials found"
          fi

      - name: Discover available tables
        run: |
          python -c '
          import os
          import requests
          import json
          import re
          
          supabase_url = os.environ["SUPABASE_URL"]
          supabase_key = os.environ["SUPABASE_KEY"]
          
          headers = {
              "apikey": supabase_key,
              "Authorization": f"Bearer {supabase_key}",
              "Content-Type": "application/json"
          }
          
          print("üîç Discovering available tables...")
          
          # Get the OpenAPI spec which contains all available endpoints
          try:
              response = requests.get(f"{supabase_url}/rest/v1/", headers=headers)
              
              if response.status_code == 200:
                  spec = response.json()
                  
                  # Extract table names from the paths
                  tables = []
                  if "paths" in spec:
                      for path in spec["paths"].keys():
                          # Remove leading slash and extract table name
                          if path.startswith("/") and len(path) > 1:
                              table_name = path[1:]  # Remove leading slash
                              # Skip paths with parameters or special endpoints
                              if not any(char in table_name for char in ["?", "{", "}", "rpc/"]):
                                  tables.append(table_name)
                  
                  print(f"üìã Found {len(tables)} tables:")
                  for table in sorted(tables):
                      print(f"  - {table}")
                  
                  # Test access to each table
                  print("\nüîç Testing table access:")
                  accessible_tables = []
                  
                  for table in tables:
                      try:
                          test_response = requests.get(
                              f"{supabase_url}/rest/v1/{table}?limit=1",
                              headers=headers
                          )
                          
                          if test_response.status_code == 200:
                              data = test_response.json()
                              row_count = len(data)
                              print(f"  ‚úÖ {table} - accessible ({row_count} sample row{'s' if row_count != 1 else ''})")
                              accessible_tables.append(table)
                          elif test_response.status_code == 401:
                              print(f"  üîí {table} - authentication required")
                          elif test_response.status_code == 403:
                              print(f"  üö´ {table} - access forbidden (check RLS policies)")
                          else:
                              print(f"  ‚ùå {table} - error: {test_response.status_code}")
                              
                      except Exception as e:
                          print(f"  ‚ùå {table} - error: {str(e)}")
                  
                  print(f"\n‚úÖ {len(accessible_tables)} tables are accessible:")
                  for table in accessible_tables:
                      print(f"  - {table}")
                      
                  # Save accessible tables to a file for the next step
                  with open("accessible_tables.txt", "w") as f:
                      for table in accessible_tables:
                          f.write(f"{table}\n")
                          
              else:
                  print(f"‚ùå Failed to get OpenAPI spec: {response.status_code}")
                  
          except Exception as e:
              print(f"‚ùå Error during discovery: {str(e)}")
          '

      - name: Export data from Supabase
        run: |
          mkdir -p exports
          python -c '
          import os
          import requests
          import pandas as pd
          import json
          from datetime import datetime, timedelta
          import pytz
          
          # Configuration
          supabase_url = os.environ["SUPABASE_URL"]
          supabase_key = os.environ["SUPABASE_KEY"]
          
          # Common headers for all requests
          headers = {
              "apikey": supabase_key,
              "Authorization": f"Bearer {supabase_key}",
              "Content-Type": "application/json",
              "Prefer": "return=representation"
          }
          
          def fetch_data(table_name, query_params=None, select_fields="*"):
              """Fetch data from Supabase table and return as DataFrame"""
              endpoint = f"{supabase_url}/rest/v1/{table_name}"
              
              params = {}
              if select_fields != "*":
                  params["select"] = select_fields
                  
              if query_params:
                  params.update(query_params)
                  
              try:
                  print(f"üîç Fetching from {table_name} with params {params}")
                  response = requests.get(endpoint, headers=headers, params=params)
                  
                  if response.status_code == 200:
                      data = response.json()
                      if not data:
                          print(f"üì≠ No data returned for {table_name}")
                          return pd.DataFrame()
                          
                      df = pd.DataFrame(data)
                      print(f"‚úÖ Successfully fetched {len(df)} rows from {table_name}")
                      return df
                  else:
                      print(f"‚ùå Error fetching {table_name}: {response.status_code} - {response.text}")
                      return pd.DataFrame()
                      
              except Exception as e:
                  print(f"‚ùå Error fetching {table_name}: {str(e)}")
                  return pd.DataFrame()
          
          # Read accessible tables from discovery step
          accessible_tables = []
          try:
              with open("accessible_tables.txt", "r") as f:
                  accessible_tables = [line.strip() for line in f.readlines() if line.strip()]
              print(f"üìã Found {len(accessible_tables)} accessible tables: {accessible_tables}")
          except FileNotFoundError:
              print("‚ö†Ô∏è accessible_tables.txt not found, using fallback table names")
              accessible_tables = ["companies", "users", "projects"]  # fallback based on discovery results
          
          # Calculate date for filtering time entries (last 30 days)
          thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
          current_date = datetime.now().strftime("%Y-%m-%d")
          
          # Map logical names to actual table names found in Supabase
          table_mapping = {}
          
          # Try to match discovered tables to our expected data types
          for table in accessible_tables:
              table_lower = table.lower()
              if "compan" in table_lower:
                  table_mapping["companies"] = table
              elif "project" in table_lower:
                  table_mapping["projects"] = table
              elif "user" in table_lower:
                  table_mapping["users"] = table
              elif "activity" in table_lower and "type" in table_lower:
                  table_mapping["activity_types"] = table
              elif "focus" in table_lower and "area" in table_lower:
                  table_mapping["focus_areas"] = table
              elif "time" in table_lower and ("entr" in table_lower or "entry" in table_lower):
                  table_mapping["time_entries"] = table
              elif "submission" in table_lower:
                  table_mapping["submissions"] = table
          
          print(f"üó∫Ô∏è Table mapping: {table_mapping}")
          
          # Export data for each mapped table
          results = {}
          
          for logical_name, actual_table_name in table_mapping.items():
              print(f"\nüìä Processing {logical_name} ({actual_table_name})...")
              
              try:
                  # Different filtering for different table types
                  if logical_name == "time_entries":
                      # Time entries with date filtering
                      df = fetch_data(actual_table_name, 
                                     {"deletedAt": "is.null", 
                                      "entryDate": f"gte.{thirty_days_ago}"})
                  else:
                      # Other tables with standard filtering
                      df = fetch_data(actual_table_name, {"deletedAt": "is.null"})
                  
                  results[logical_name] = len(df) if not df.empty else 0
                  
                  if not df.empty:
                      # Remove sensitive fields for users
                      if logical_name == "users" and "password" in df.columns:
                          df = df.drop(columns=["password"])
                          print("üîí Removed password column from users data")
                      
                      # Save to CSV
                      csv_filename = f"exports/{logical_name}.csv"
                      df.to_csv(csv_filename, index=False)
                      print(f"üíæ Saved {len(df)} rows to {csv_filename}")
                  else:
                      print(f"üì≠ No data found for {logical_name}")
                      
              except Exception as e:
                  print(f"‚ùå Error processing {logical_name}: {str(e)}")
                  results[logical_name] = 0
          
          # Also export any unmatched tables that might contain useful data
          unmatched_tables = [t for t in accessible_tables if t not in table_mapping.values()]
          print(f"\nüîç Found {len(unmatched_tables)} additional tables: {unmatched_tables}")
          
          for table_name in unmatched_tables:
              try:
                  print(f"\nüìä Processing additional table: {table_name}")
                  df = fetch_data(table_name, {"limit": "1000"})  # Limit to prevent huge exports
                  
                  if not df.empty:
                      results[f"additional_{table_name}"] = len(df)
                      csv_filename = f"exports/additional_{table_name}.csv"
                      df.to_csv(csv_filename, index=False)
                      print(f"üíæ Saved {len(df)} rows to {csv_filename}")
                  else:
                      results[f"additional_{table_name}"] = 0
                      
              except Exception as e:
                  print(f"‚ùå Error processing additional table {table_name}: {str(e)}")
                  results[f"additional_{table_name}"] = 0
          
          # Create a summary file with export information
          with open("exports/export_summary.json", "w") as f:
              summary = {
                  "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                  "accessible_tables": accessible_tables,
                  "table_mapping": table_mapping,
                  "tables_exported": results,
                  "time_entries_period": f"{thirty_days_ago} to {current_date}"
              }
              json.dump(summary, f, indent=2)
          
          # Get current timestamp for HTML
          timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
          
          # Create an index.html file for GitHub Pages
          html_content = f"""<!DOCTYPE html>
          <html>
          <head>
              <title>Supabase Data Exports</title>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1">
              <style>
                  body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; line-height: 1.6; }}
                  h1 {{ color: #333; }}
                  .container {{ max-width: 800px; margin: 0 auto; }}
                  table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                  th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                  th {{ background-color: #f2f2f2; }}
                  .download-btn {{ display: inline-block; background-color: #4CAF50; color: white; 
                                  padding: 8px 16px; text-decoration: none; border-radius: 4px; }}
                  .download-btn:hover {{ background-color: #45a049; }}
                  .timestamp {{ color: #666; font-style: italic; }}
                  .section {{ margin: 30px 0; }}
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>Supabase Data Exports</h1>
                  <p class="timestamp">Last updated: {timestamp} UTC</p>
                  
                  <div class="section">
                      <h2>Main Data Tables</h2>
                      <table>
                          <tr>
                              <th>Dataset</th>
                              <th>Records</th>
                              <th>Download</th>
                          </tr>"""
          
          # Add main table rows
          main_tables = ["companies", "projects", "users", "activity_types", "focus_areas", "time_entries", "submissions"]
          for table_name in main_tables:
              if table_name in results:
                  count = results[table_name]
                  file_name = f"{table_name}.csv"
                  display_name = table_name.replace("_", " ").title()
                  
                  if table_name == "time_entries":
                      display_name += " (Last 30 Days)"
                      
                  html_content += f"""
                          <tr>
                              <td>{display_name}</td>
                              <td>{count}</td>
                              <td>"""
                  
                  if count > 0:
                      html_content += f"""<a href="{file_name}" class="download-btn">Download CSV</a>"""
                  else:
                      html_content += """<span style="color: #999;">No data</span>"""
                      
                  html_content += """</td>
                          </tr>"""
          
          html_content += """
                      </table>
                  </div>"""
          
          # Add additional tables if any
          additional_tables = [k for k in results.keys() if k.startswith("additional_")]
          if additional_tables:
              html_content += """
                  <div class="section">
                      <h2>Additional Tables</h2>
                      <table>
                          <tr>
                              <th>Dataset</th>
                              <th>Records</th>
                              <th>Download</th>
                          </tr>"""
              
              for table_name in additional_tables:
                  count = results[table_name]
                  file_name = f"{table_name}.csv"
                  display_name = table_name.replace("additional_", "").replace("_", " ").title()
                  
                  html_content += f"""
                          <tr>
                              <td>{display_name}</td>
                              <td>{count}</td>
                              <td>"""
                  
                  if count > 0:
                      html_content += f"""<a href="{file_name}" class="download-btn">Download CSV</a>"""
                  else:
                      html_content += """<span style="color: #999;">No data</span>"""
                      
                  html_content += """</td>
                          </tr>"""
              
              html_content += """
                      </table>
                  </div>"""
          
          # Complete the HTML
          html_content += f"""
                  <div class="section">
                      <h2>Export Details</h2>
                      <p><strong>Time period for time entries:</strong> {thirty_days_ago} to {current_date}</p>
                      <p><strong>Tables discovered:</strong> {len(accessible_tables)}</p>
                      <p><strong>Tables exported:</strong> {sum(1 for count in results.values() if count > 0)}</p>
                      <p><a href="export_summary.json">View detailed export summary (JSON)</a></p>
                  </div>
              </div>
          </body>
          </html>"""
          
          with open("exports/index.html", "w") as f:
              f.write(html_content)
          
          # Final summary
          successful_exports = sum(1 for count in results.values() if count > 0)
          total_records = sum(results.values())
          
          if successful_exports == 0:
              print("\n‚ö†Ô∏è Warning: No data was successfully exported from any table")
              print("Please check your Supabase configuration and table access permissions")
          else:
              print(f"\n‚úÖ Successfully exported {total_records} records from {successful_exports} tables")
              print("üìÑ Generated index.html for GitHub Pages")
          '
